{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c2c4b1-3520-4717-b03c-e7cf3ed395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT = \"-O3\"\n",
    "#Note that the templates for Falcon512 and Falcon1024 are universal because they are templates built for each coefficient.\n",
    "#Don't be misled by the \"512\" here.\n",
    "Falcon_n_template = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aeab956-a81e-4f48-b367-5ef249ad13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sca_preprocess import trace_categorize\n",
    "from sca_preprocess import trace_fft, trace_butter_lpf\n",
    "from sca_preprocess import calc_snr, calc_ttest, calc_sod\n",
    "from sca_preprocess import select_poi_max_rank, select_poi_threshold\n",
    "from sca_preprocess import template_build, template_attack, template_attack_report\n",
    "from TA_discriminative import classifier_train, classifier_inference, classifier_report\n",
    "\n",
    "MCU_freq = 30e6\n",
    "adc_mul = 4\n",
    "sample_freq = MCU_freq * adc_mul\n",
    "cutoff_freq = sample_freq / 8\n",
    "\n",
    "seed = 42\n",
    "test_split = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752e70b0-4f8b-4208-a94b-c92044553979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_traces_accto_class(traces_dict, class_label_map):\n",
    "    final_traces = []\n",
    "    label = []\n",
    "    for cls in range(len(class_label_map.keys())):\n",
    "        N_trace = class_label_map[cls]['num_to_select']\n",
    "        label_range = class_label_map[cls]['labels']\n",
    "        traces_in_class = []\n",
    "        for l in label_range:\n",
    "            traces_in_class.append(traces_dict[l])\n",
    "            # print(l, traces_in_class[-1].shape)\n",
    "        combined = np.concatenate(traces_in_class, axis=0)  # ((1/2/4/8/16) x #trace, #sample)\n",
    "        # (1/2/4/8/16) x #trace ⪭ #trace，random pick #trace from it\n",
    "        selected_idx = np.random.choice(combined.shape[0], N_trace, replace=False)\n",
    "        selected = combined[selected_idx]\n",
    "        final_traces.append(selected)\n",
    "        label += [cls] * N_trace\n",
    "    traces = np.vstack(final_traces)  # shape (#class * #trace, #sample)\n",
    "    label = np.array(label)  # (#class * #trace,)\n",
    "    return traces, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99934aa8-2f5f-4293-a80c-2ece6e1731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_split(traces_dict, class_label_map, start_idx, end_idx, test_split,\n",
    "                     seed=seed, sample_freq=sample_freq, cutoff_freq=cutoff_freq):\n",
    "    # select traces in class\n",
    "    np.random.seed(seed)\n",
    "    traces, label = select_traces_accto_class(traces_dict, class_label_map)\n",
    "    traces = traces[:, start_idx:end_idx]\n",
    "    traces_lpf = trace_butter_lpf(traces, sample_freq, cutoff_freq)\n",
    "    # traces_dict = trace_categorize(traces, label)\n",
    "    print(f\"traces.shape: {traces.shape}\")\n",
    "    print(f\"label.shape: {label.shape}\")\n",
    "\n",
    "    # train/test split\n",
    "    traces_train, traces_test, label_train, label_test = \\\n",
    "        train_test_split(traces_lpf, label,\n",
    "                         test_size=test_split,\n",
    "                         stratify=label,\n",
    "                         shuffle=True,\n",
    "                         random_state=seed)\n",
    "    print(f\"traces_train.shape: {traces_train.shape}\")\n",
    "    print(f\"traces_test.shape: {traces_test.shape}\")\n",
    "\n",
    "    return traces_train, traces_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538a53e3-36a3-47fc-8b1a-54b53d0a9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_indices = {\n",
    "    'scaled': (0, 121),\n",
    "    'mul_shift': (41, 63),\n",
    "    'mul_multiply': (22, 39)\n",
    "}\n",
    "N_k_trans = 2\n",
    "N_trace = Falcon_n_template * N_k_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6d9bf2-c742-41de-9e47-257639b3e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(N_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9262b638-ad40-4fab-9cb5-6bff77726c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect #trace/label = 100\n",
    "\n",
    "class_label_map_scaled = {\n",
    "    0: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [0]},\n",
    "    1: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [1]},\n",
    "    2: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-1]},\n",
    "    3: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [2, 3]},\n",
    "    4: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-3, -2]},\n",
    "    5: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [4, 5, 6, 7]},\n",
    "    6: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-7, -6, -5, -4]},\n",
    "    7: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(8, 16))},    # [8, ..., 15]\n",
    "    8: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(-15, -7))},  # [-15, ..., -8]\n",
    "    # 9: {\"num_to_select\": N_trace,\n",
    "    #     \"labels\": list(range(16, 23))},    # [16, ..., 22]\n",
    "    # 10: {\"num_to_select\": N_trace,\n",
    "    #     \"labels\": list(range(-22, -15))},  # [-22, ..., -16]\n",
    "}\n",
    "# N_class = len(class_label_map_scaled.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a82988-cf67-4425-a03e-cae4d052d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the key distribution, it is virtually impossible to obtain a value larger than 15 in Falcon1024.\n",
    "class_label_map_shift = {\n",
    "    0: {\"num_to_select\": N_trace * 7,  # b.c. class 0 has 14 elements\n",
    "        \"labels\":  # make zu<2^55\n",
    "        # [2, 4, 5, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22]},\n",
    "    [2, 4, 5, 8, 9, 10, 11]},\n",
    "    1: {\"num_to_select\": N_trace * 7,  # b.c. class 1 has 7 elements\n",
    "        \"labels\":  # make zu>=2^55\n",
    "        # [3, 6, 7, 12, 13, 14, 15, 23, 24, 25, 26, 27, 28, 29, 30, 31]},\n",
    "        [3, 6, 7, 12, 13, 14, 15]},\n",
    "}\n",
    "\n",
    "class_label_map_4_5 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [4]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [5]},\n",
    "}\n",
    "class_label_map_6_7 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [6]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [7]},\n",
    "}\n",
    "class_label_map_8_11 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [8]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [9]},\n",
    "    2: {\"num_to_select\": N_trace, \"labels\": [10]},\n",
    "    3: {\"num_to_select\": N_trace, \"labels\": [11]},\n",
    "}\n",
    "class_label_map_12_15 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [12]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [13]},\n",
    "    2: {\"num_to_select\": N_trace, \"labels\": [14]},\n",
    "    3: {\"num_to_select\": N_trace, \"labels\": [15]},\n",
    "}\n",
    "# class_label_map_16_22 = {\n",
    "#     0: {\"num_to_select\": N_trace, \"labels\": [16]},\n",
    "#     1: {\"num_to_select\": N_trace, \"labels\": [17]},\n",
    "#     2: {\"num_to_select\": N_trace, \"labels\": [18]},\n",
    "#     3: {\"num_to_select\": N_trace, \"labels\": [19]},\n",
    "#     4: {\"num_to_select\": N_trace, \"labels\": [20]},\n",
    "#     5: {\"num_to_select\": N_trace, \"labels\": [21]},\n",
    "#     6: {\"num_to_select\": N_trace, \"labels\": [22]},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8fb7abd-70f4-4ac2-9038-5a3f5dc95f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_from_csv(file_name, target_row, repeat_times, class_label_map):\n",
    "    \n",
    "    value_to_key = {v: k for k, values in class_label_map.items() for v in values['labels']}\n",
    "    \n",
    "    df = pd.read_csv(file_name, header=None, skiprows=target_row, nrows=1)\n",
    "    \n",
    "    labels_2d = df.map(lambda x: value_to_key[x]).to_numpy()  # shape (1, 512)\n",
    "    \n",
    "    labels_repeated_2d = np.tile(labels_2d, (1, repeat_times))  # shape (1, 512*repeat_times)\n",
    "    \n",
    "    labels_scaled_key = labels_repeated_2d.flatten()\n",
    "    return labels_scaled_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875b59f6-17dd-4bc9-b264-66650f4a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel(nn.Module):\n",
    "    def __init__(self, N_class):\n",
    "        super().__init__()\n",
    "        k = 5\n",
    "        p = k // 2  # \"same\" padding\n",
    "        N_channel = 1\n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv1d(N_channel, 4, kernel_size=k, padding=p)\n",
    "        self.conv2 = nn.Conv1d(4, 4, kernel_size=k, padding=p)\n",
    "        self.bn1 = nn.BatchNorm1d(4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # MLP\n",
    "        self.fc1 = nn.LazyLinear(128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, N_class)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.act(self.bn2(self.conv2(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94288e1f-9ff1-44c2-854a-17cb9cf3054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fpr_templates_ml(traces_dict, clip_range,\n",
    "                           test_split, class_label_map, num_epochs=50):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # start_idx = start_end_idx[0]\n",
    "    # end_idx   = start_end_idx[1]\n",
    "    # clip_range = list(range((end_idx - start_idx) * 4))\n",
    "\n",
    "    traces_train, traces_test, label_train, label_test = \\\n",
    "        select_and_split(traces_dict, class_label_map,\n",
    "                         0, clip_range[-1]+1, test_split)\n",
    "    N_class = len(class_label_map.keys())\n",
    "    model = NaiveModel(N_class)\n",
    "    model, scaler = classifier_train(traces_train, label_train,\\\n",
    "                                     clip_range, model)\n",
    "    y_prob = classifier_inference(traces_test,\n",
    "                             clip_range, model, scaler)\n",
    "    classifier_report(traces_test, label_test, clip_range, model, scaler,\n",
    "                     verbose=True)\n",
    "    # we actually don't need y_prob to attack; in case you need it to check bug, I also export it\n",
    "    return model, scaler, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ab53e6-2fba-4a77-9295-7debe606e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "for i in range(-15, 16):\n",
    "    label_all.extend([i] * N_trace)\n",
    "label_all = np.array(label_all)  # #trace x 45 (-22 to 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1db718c-935b-451c-9a81-0939fcd1075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_scaled = []\n",
    "len_scaled = 121 * 4\n",
    "for i in range(-15, 16):\n",
    "    traces_scaled.append(np.load(f'data{OPT}-828/profile/scaled/'\n",
    "                                 f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_scaled))\n",
    "traces_scaled = np.array(traces_scaled).reshape(-1, len_scaled)\n",
    "traces_scaled_dict = trace_categorize(traces_scaled, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d3309e-eb58-4afa-9035-3e3a5fd61e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (9216, 484)\n",
      "label.shape: (9216,)\n",
      "traces_train.shape: (4608, 484)\n",
      "traces_test.shape: (4608, 484)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            1,161\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,797\n",
      "Trainable params: 17,797\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.8701 acc 0.4502 | val loss 1.4627 acc 0.8460\n",
      "Epoch 02 | train loss 0.5782 acc 0.9265 | val loss 0.1398 acc 0.9892\n",
      "Epoch 03 | train loss 0.1120 acc 0.9819 | val loss 0.0500 acc 0.9913\n",
      "Epoch 04 | train loss 0.0591 acc 0.9896 | val loss 0.0310 acc 0.9957\n",
      "Epoch 05 | train loss 0.0395 acc 0.9923 | val loss 0.0266 acc 0.9892\n",
      "Epoch 06 | train loss 0.0301 acc 0.9952 | val loss 0.0200 acc 0.9935\n",
      "Epoch 07 | train loss 0.0240 acc 0.9954 | val loss 0.0179 acc 0.9978\n",
      "Epoch 08 | train loss 0.0218 acc 0.9947 | val loss 0.0137 acc 0.9978\n",
      "Epoch 09 | train loss 0.0175 acc 0.9957 | val loss 0.0134 acc 0.9978\n",
      "Epoch 10 | train loss 0.0154 acc 0.9966 | val loss 0.0223 acc 0.9913\n",
      "Epoch 11 | train loss 0.0123 acc 0.9981 | val loss 0.0201 acc 0.9892\n",
      "Epoch 12 | train loss 0.0098 acc 0.9983 | val loss 0.0246 acc 0.9870\n",
      "Epoch 13 | train loss 0.0201 acc 0.9945 | val loss 0.0108 acc 0.9978\n",
      "Epoch 14 | train loss 0.0185 acc 0.9935 | val loss 0.0082 acc 0.9978\n",
      "Epoch 15 | train loss 0.0054 acc 0.9993 | val loss 0.0030 acc 1.0000\n",
      "Epoch 16 | train loss 0.0046 acc 0.9990 | val loss 0.0025 acc 1.0000\n",
      "Epoch 17 | train loss 0.0046 acc 0.9990 | val loss 0.0022 acc 1.0000\n",
      "Epoch 18 | train loss 0.0039 acc 0.9995 | val loss 0.0042 acc 1.0000\n",
      "Epoch 19 | train loss 0.0025 acc 1.0000 | val loss 0.0024 acc 1.0000\n",
      "Epoch 20 | train loss 0.0028 acc 0.9998 | val loss 0.0027 acc 1.0000\n",
      "Epoch 21 | train loss 0.0029 acc 0.9998 | val loss 0.0017 acc 1.0000\n",
      "Epoch 22 | train loss 0.0028 acc 0.9995 | val loss 0.0027 acc 1.0000\n",
      "Epoch 23 | train loss 0.0027 acc 0.9995 | val loss 0.0017 acc 1.0000\n",
      "Epoch 24 | train loss 0.0020 acc 0.9998 | val loss 0.0029 acc 1.0000\n",
      "Epoch 25 | train loss 0.0020 acc 0.9998 | val loss 0.0013 acc 1.0000\n",
      "Epoch 26 | train loss 0.0014 acc 1.0000 | val loss 0.0009 acc 1.0000\n",
      "Epoch 27 | train loss 0.0018 acc 0.9995 | val loss 0.0006 acc 1.0000\n",
      "Epoch 28 | train loss 0.0027 acc 0.9995 | val loss 0.0055 acc 0.9978\n",
      "Epoch 29 | train loss 0.0024 acc 0.9995 | val loss 0.0026 acc 1.0000\n",
      "Epoch 30 | train loss 0.0018 acc 0.9995 | val loss 0.0005 acc 1.0000\n",
      "Epoch 31 | train loss 0.0008 acc 1.0000 | val loss 0.0007 acc 1.0000\n",
      "Epoch 32 | train loss 0.0010 acc 1.0000 | val loss 0.0009 acc 1.0000\n",
      "Epoch 33 | train loss 0.0011 acc 1.0000 | val loss 0.0011 acc 1.0000\n",
      "Epoch 34 | train loss 0.0009 acc 1.0000 | val loss 0.0007 acc 1.0000\n",
      "Epoch 35 | train loss 0.0015 acc 0.9995 | val loss 0.0004 acc 1.0000\n",
      "Epoch 36 | train loss 0.0013 acc 0.9995 | val loss 0.0027 acc 1.0000\n",
      "Epoch 37 | train loss 0.0012 acc 1.0000 | val loss 0.0020 acc 1.0000\n",
      "Epoch 38 | train loss 0.0016 acc 0.9995 | val loss 0.0006 acc 1.0000\n",
      "Epoch 39 | train loss 0.0018 acc 0.9995 | val loss 0.0017 acc 1.0000\n",
      "Epoch 40 | train loss 0.0011 acc 1.0000 | val loss 0.0006 acc 1.0000\n",
      "Epoch 41 | train loss 0.0028 acc 0.9990 | val loss 0.0011 acc 1.0000\n",
      "Epoch 42 | train loss 0.0022 acc 0.9988 | val loss 0.0011 acc 1.0000\n",
      "Epoch 43 | train loss 0.0011 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 44 | train loss 0.0007 acc 1.0000 | val loss 0.0005 acc 1.0000\n",
      "Epoch 45 | train loss 0.0011 acc 0.9995 | val loss 0.0046 acc 0.9978\n",
      "Epoch 46 | train loss 0.0047 acc 0.9983 | val loss 0.0014 acc 1.0000\n",
      "Epoch 47 | train loss 0.0016 acc 1.0000 | val loss 0.0005 acc 1.0000\n",
      "Epoch 48 | train loss 0.0006 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 49 | train loss 0.0004 acc 1.0000 | val loss 0.0004 acc 1.0000\n",
      "Epoch 50 | train loss 0.0003 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Test set LOSS: 0.0003\n",
      "Test set ACCURACY: 1.0000\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   1.000000  1.000000  1.000000       512\n",
      "           1   1.000000  1.000000  1.000000       512\n",
      "           2   1.000000  1.000000  1.000000       512\n",
      "           3   1.000000  1.000000  1.000000       512\n",
      "           4   1.000000  1.000000  1.000000       512\n",
      "           5   1.000000  1.000000  1.000000       512\n",
      "           6   1.000000  1.000000  1.000000       512\n",
      "           7   1.000000  1.000000  1.000000       512\n",
      "           8   1.000000  1.000000  1.000000       512\n",
      "\n",
      "    accuracy                       1.000000      4608\n",
      "   macro avg   1.000000  1.000000  1.000000      4608\n",
      "weighted avg   1.000000  1.000000  1.000000      4608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_scaled, scaler_scaled, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_scaled_dict, range(len_scaled),\n",
    "                           test_split, class_label_map_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19b6d95c-8627-4554-ac5f-d871b747c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_shift = []\n",
    "len_shift = 22 * 4\n",
    "for i in range(-15, 16):\n",
    "    traces_shift.append(np.load(f'data{OPT}-828/profile/shift/'\n",
    "                                f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_shift))\n",
    "traces_shift = np.array(traces_shift).reshape(-1, len_shift)\n",
    "traces_shift_dict = trace_categorize(traces_shift, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb76d2af-fdb2-4a58-9219-c2e3ddeb9ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (14336, 88)\n",
      "label.shape: (14336,)\n",
      "traces_train.shape: (7168, 88)\n",
      "traces_test.shape: (7168, 88)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.6393 acc 0.6477 | val loss 0.5628 acc 0.7155\n",
      "Epoch 02 | train loss 0.5030 acc 0.7583 | val loss 0.4691 acc 0.7880\n",
      "Epoch 03 | train loss 0.4396 acc 0.8005 | val loss 0.4285 acc 0.8159\n",
      "Epoch 04 | train loss 0.4085 acc 0.8154 | val loss 0.4000 acc 0.8201\n",
      "Epoch 05 | train loss 0.3861 acc 0.8315 | val loss 0.3802 acc 0.8312\n",
      "Epoch 06 | train loss 0.3657 acc 0.8413 | val loss 0.3867 acc 0.8285\n",
      "Epoch 07 | train loss 0.3548 acc 0.8512 | val loss 0.3741 acc 0.8298\n",
      "Epoch 08 | train loss 0.3544 acc 0.8470 | val loss 0.3470 acc 0.8424\n",
      "Epoch 09 | train loss 0.3390 acc 0.8561 | val loss 0.3555 acc 0.8438\n",
      "Epoch 10 | train loss 0.3399 acc 0.8577 | val loss 0.3453 acc 0.8466\n",
      "Epoch 11 | train loss 0.3281 acc 0.8639 | val loss 0.3179 acc 0.8703\n",
      "Epoch 12 | train loss 0.3189 acc 0.8672 | val loss 0.3377 acc 0.8550\n",
      "Epoch 13 | train loss 0.3172 acc 0.8684 | val loss 0.3111 acc 0.8689\n",
      "Epoch 14 | train loss 0.3107 acc 0.8706 | val loss 0.3043 acc 0.8661\n",
      "Epoch 15 | train loss 0.2964 acc 0.8805 | val loss 0.2944 acc 0.8689\n",
      "Epoch 16 | train loss 0.3020 acc 0.8763 | val loss 0.2959 acc 0.8675\n",
      "Epoch 17 | train loss 0.2942 acc 0.8774 | val loss 0.2961 acc 0.8773\n",
      "Epoch 18 | train loss 0.2874 acc 0.8796 | val loss 0.2956 acc 0.8703\n",
      "Epoch 19 | train loss 0.2822 acc 0.8836 | val loss 0.2748 acc 0.8745\n",
      "Epoch 20 | train loss 0.2796 acc 0.8830 | val loss 0.3333 acc 0.8522\n",
      "Epoch 21 | train loss 0.2777 acc 0.8878 | val loss 0.2604 acc 0.8884\n",
      "Epoch 22 | train loss 0.2657 acc 0.8884 | val loss 0.2653 acc 0.8856\n",
      "Epoch 23 | train loss 0.2537 acc 0.8986 | val loss 0.2540 acc 0.8856\n",
      "Epoch 24 | train loss 0.2519 acc 0.8977 | val loss 0.2605 acc 0.8884\n",
      "Epoch 25 | train loss 0.2492 acc 0.8997 | val loss 0.2532 acc 0.8856\n",
      "Epoch 26 | train loss 0.2481 acc 0.8965 | val loss 0.2536 acc 0.8954\n",
      "Epoch 27 | train loss 0.2443 acc 0.9011 | val loss 0.2489 acc 0.8926\n",
      "Epoch 28 | train loss 0.2392 acc 0.9037 | val loss 0.2402 acc 0.8940\n",
      "Epoch 29 | train loss 0.2356 acc 0.9054 | val loss 0.2364 acc 0.8954\n",
      "Epoch 30 | train loss 0.2297 acc 0.9076 | val loss 0.2421 acc 0.8954\n",
      "Epoch 31 | train loss 0.2294 acc 0.9062 | val loss 0.2379 acc 0.8954\n",
      "Epoch 32 | train loss 0.2256 acc 0.9096 | val loss 0.2419 acc 0.8926\n",
      "Epoch 33 | train loss 0.2233 acc 0.9102 | val loss 0.2475 acc 0.9010\n",
      "Epoch 34 | train loss 0.2260 acc 0.9092 | val loss 0.2552 acc 0.8815\n",
      "Epoch 35 | train loss 0.2185 acc 0.9132 | val loss 0.2425 acc 0.8940\n",
      "Epoch 36 | train loss 0.2295 acc 0.9065 | val loss 0.2566 acc 0.8912\n",
      "Epoch 37 | train loss 0.2167 acc 0.9154 | val loss 0.2772 acc 0.8870\n",
      "Epoch 38 | train loss 0.2243 acc 0.9051 | val loss 0.2544 acc 0.9024\n",
      "Epoch 39 | train loss 0.2069 acc 0.9175 | val loss 0.2432 acc 0.9066\n",
      "Epoch 40 | train loss 0.2071 acc 0.9164 | val loss 0.2254 acc 0.9024\n",
      "Epoch 41 | train loss 0.2119 acc 0.9155 | val loss 0.2327 acc 0.9038\n",
      "Epoch 42 | train loss 0.2033 acc 0.9172 | val loss 0.2343 acc 0.9038\n",
      "Epoch 43 | train loss 0.2003 acc 0.9172 | val loss 0.2258 acc 0.9079\n",
      "Epoch 44 | train loss 0.2017 acc 0.9195 | val loss 0.2340 acc 0.9024\n",
      "Epoch 45 | train loss 0.1933 acc 0.9226 | val loss 0.2314 acc 0.9010\n",
      "Epoch 46 | train loss 0.1996 acc 0.9202 | val loss 0.2252 acc 0.9093\n",
      "Epoch 47 | train loss 0.1971 acc 0.9208 | val loss 0.2196 acc 0.9093\n",
      "Epoch 48 | train loss 0.1928 acc 0.9236 | val loss 0.2436 acc 0.8940\n",
      "Epoch 49 | train loss 0.1874 acc 0.9256 | val loss 0.2422 acc 0.9052\n",
      "Epoch 50 | train loss 0.1890 acc 0.9257 | val loss 0.2167 acc 0.9079\n",
      "Test set LOSS: 0.2258\n",
      "Test set ACCURACY: 0.9103\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.898618  0.924944  0.911591      3584\n",
      "           1   0.922679  0.895647  0.908962      3584\n",
      "\n",
      "    accuracy                       0.910296      7168\n",
      "   macro avg   0.910648  0.910296  0.910277      7168\n",
      "weighted avg   0.910648  0.910296  0.910277      7168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_shift, scaler_shift, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_shift_dict, range(len_shift),\n",
    "                           test_split, class_label_map_shift, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f1aaf4-b20c-4ce5-b375-abb8e25c810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_mul = []\n",
    "len_mul = 17 * 4\n",
    "for i in range(-15, 16):\n",
    "    traces_mul.append(np.load(f'data{OPT}-828/profile/mul/'\n",
    "                              f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_mul))\n",
    "traces_mul = np.array(traces_mul).reshape(-1, len_mul)\n",
    "traces_mul_dict = trace_categorize(traces_mul, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "436bbc82-7313-47e1-943e-1f5304b683eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (2048, 68)\n",
      "label.shape: (2048,)\n",
      "traces_train.shape: (1024, 68)\n",
      "traces_test.shape: (1024, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.6562 acc 0.7611 | val loss 0.6770 acc 0.8058\n",
      "Epoch 02 | train loss 0.5186 acc 0.9805 | val loss 0.5908 acc 0.9709\n",
      "Epoch 03 | train loss 0.2900 acc 0.9848 | val loss 0.3438 acc 0.9806\n",
      "Epoch 04 | train loss 0.1090 acc 0.9891 | val loss 0.1104 acc 0.9903\n",
      "Epoch 05 | train loss 0.0418 acc 0.9924 | val loss 0.0571 acc 0.9903\n",
      "Epoch 06 | train loss 0.0272 acc 0.9924 | val loss 0.0555 acc 0.9903\n",
      "Epoch 07 | train loss 0.0183 acc 0.9946 | val loss 0.0617 acc 0.9903\n",
      "Epoch 08 | train loss 0.0170 acc 0.9946 | val loss 0.0628 acc 0.9903\n",
      "Epoch 09 | train loss 0.0121 acc 0.9967 | val loss 0.0608 acc 0.9903\n",
      "Epoch 10 | train loss 0.0131 acc 0.9957 | val loss 0.0721 acc 0.9903\n",
      "Epoch 11 | train loss 0.0150 acc 0.9967 | val loss 0.0729 acc 0.9903\n",
      "Epoch 12 | train loss 0.0121 acc 0.9957 | val loss 0.0653 acc 0.9903\n",
      "Epoch 13 | train loss 0.0086 acc 0.9978 | val loss 0.0773 acc 0.9903\n",
      "Epoch 14 | train loss 0.0099 acc 0.9967 | val loss 0.0626 acc 0.9903\n",
      "Epoch 15 | train loss 0.0134 acc 0.9957 | val loss 0.0821 acc 0.9903\n",
      "Epoch 16 | train loss 0.0087 acc 0.9978 | val loss 0.0658 acc 0.9903\n",
      "Epoch 17 | train loss 0.0096 acc 0.9967 | val loss 0.0716 acc 0.9903\n",
      "Epoch 18 | train loss 0.0039 acc 1.0000 | val loss 0.0785 acc 0.9903\n",
      "Epoch 19 | train loss 0.0095 acc 0.9957 | val loss 0.0758 acc 0.9903\n",
      "Epoch 20 | train loss 0.0070 acc 0.9978 | val loss 0.0816 acc 0.9903\n",
      "Epoch 21 | train loss 0.0075 acc 0.9978 | val loss 0.0750 acc 0.9903\n",
      "Epoch 22 | train loss 0.0076 acc 0.9967 | val loss 0.0836 acc 0.9903\n",
      "Early stopping triggered.\n",
      "Test set LOSS: 0.0386\n",
      "Test set ACCURACY: 0.9844\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.982490  0.986328  0.984405       512\n",
      "           1   0.986275  0.982422  0.984344       512\n",
      "\n",
      "    accuracy                       0.984375      1024\n",
      "   macro avg   0.984382  0.984375  0.984375      1024\n",
      "weighted avg   0.984382  0.984375  0.984375      1024\n",
      "\n",
      "traces.shape: (2048, 68)\n",
      "label.shape: (2048,)\n",
      "traces_train.shape: (1024, 68)\n",
      "traces_test.shape: (1024, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.6772 acc 0.6721 | val loss 0.6870 acc 0.7670\n",
      "Epoch 02 | train loss 0.6164 acc 0.8371 | val loss 0.6584 acc 0.6311\n",
      "Epoch 03 | train loss 0.5067 acc 0.8502 | val loss 0.5733 acc 0.7379\n",
      "Epoch 04 | train loss 0.3869 acc 0.8578 | val loss 0.4337 acc 0.8058\n",
      "Epoch 05 | train loss 0.2996 acc 0.8730 | val loss 0.3719 acc 0.8155\n",
      "Epoch 06 | train loss 0.2744 acc 0.8882 | val loss 0.3518 acc 0.8252\n",
      "Epoch 07 | train loss 0.2600 acc 0.8936 | val loss 0.4078 acc 0.8350\n",
      "Epoch 08 | train loss 0.2639 acc 0.9012 | val loss 0.3891 acc 0.8350\n",
      "Epoch 09 | train loss 0.2404 acc 0.9045 | val loss 0.3259 acc 0.8544\n",
      "Epoch 10 | train loss 0.2511 acc 0.8936 | val loss 0.4369 acc 0.8155\n",
      "Epoch 11 | train loss 0.2581 acc 0.8979 | val loss 0.3352 acc 0.8544\n",
      "Epoch 12 | train loss 0.2301 acc 0.9153 | val loss 0.3334 acc 0.8447\n",
      "Epoch 13 | train loss 0.2273 acc 0.9121 | val loss 0.3492 acc 0.8350\n",
      "Epoch 14 | train loss 0.2262 acc 0.9153 | val loss 0.3104 acc 0.8641\n",
      "Epoch 15 | train loss 0.2267 acc 0.9164 | val loss 0.3779 acc 0.8447\n",
      "Epoch 16 | train loss 0.2234 acc 0.9066 | val loss 0.3555 acc 0.8350\n",
      "Epoch 17 | train loss 0.2110 acc 0.9164 | val loss 0.3272 acc 0.8544\n",
      "Epoch 18 | train loss 0.2153 acc 0.9186 | val loss 0.3522 acc 0.8447\n",
      "Epoch 19 | train loss 0.2183 acc 0.9175 | val loss 0.3596 acc 0.8447\n",
      "Epoch 20 | train loss 0.2172 acc 0.9131 | val loss 0.3010 acc 0.8641\n",
      "Epoch 21 | train loss 0.2104 acc 0.9164 | val loss 0.3546 acc 0.8447\n",
      "Epoch 22 | train loss 0.2118 acc 0.9175 | val loss 0.2922 acc 0.8641\n",
      "Epoch 23 | train loss 0.2150 acc 0.9186 | val loss 0.3659 acc 0.8447\n",
      "Epoch 24 | train loss 0.2088 acc 0.9153 | val loss 0.3056 acc 0.8447\n",
      "Epoch 25 | train loss 0.2010 acc 0.9229 | val loss 0.3104 acc 0.8447\n",
      "Epoch 26 | train loss 0.1942 acc 0.9207 | val loss 0.3260 acc 0.8447\n",
      "Epoch 27 | train loss 0.2069 acc 0.9262 | val loss 0.2925 acc 0.8641\n",
      "Epoch 28 | train loss 0.2026 acc 0.9240 | val loss 0.3251 acc 0.8447\n",
      "Epoch 29 | train loss 0.2130 acc 0.9175 | val loss 0.3634 acc 0.8544\n",
      "Epoch 30 | train loss 0.1969 acc 0.9229 | val loss 0.2977 acc 0.8835\n",
      "Epoch 31 | train loss 0.2051 acc 0.9153 | val loss 0.3104 acc 0.8544\n",
      "Epoch 32 | train loss 0.1921 acc 0.9218 | val loss 0.3302 acc 0.8447\n",
      "Epoch 33 | train loss 0.1973 acc 0.9240 | val loss 0.2909 acc 0.8835\n",
      "Epoch 34 | train loss 0.1924 acc 0.9251 | val loss 0.3572 acc 0.8447\n",
      "Epoch 35 | train loss 0.1937 acc 0.9207 | val loss 0.3159 acc 0.8544\n",
      "Epoch 36 | train loss 0.1864 acc 0.9218 | val loss 0.2988 acc 0.8835\n",
      "Epoch 37 | train loss 0.1822 acc 0.9262 | val loss 0.3284 acc 0.8544\n",
      "Epoch 38 | train loss 0.1799 acc 0.9251 | val loss 0.3018 acc 0.8835\n",
      "Epoch 39 | train loss 0.1846 acc 0.9316 | val loss 0.3158 acc 0.8738\n",
      "Epoch 40 | train loss 0.1788 acc 0.9273 | val loss 0.3377 acc 0.8738\n",
      "Epoch 41 | train loss 0.1792 acc 0.9327 | val loss 0.3084 acc 0.8738\n",
      "Epoch 42 | train loss 0.1776 acc 0.9327 | val loss 0.2930 acc 0.8835\n",
      "Epoch 43 | train loss 0.1712 acc 0.9283 | val loss 0.3002 acc 0.8835\n",
      "Epoch 44 | train loss 0.1746 acc 0.9316 | val loss 0.3073 acc 0.8835\n",
      "Epoch 45 | train loss 0.1647 acc 0.9316 | val loss 0.3010 acc 0.8835\n",
      "Epoch 46 | train loss 0.1709 acc 0.9294 | val loss 0.3178 acc 0.8932\n",
      "Epoch 47 | train loss 0.1785 acc 0.9305 | val loss 0.2793 acc 0.8835\n",
      "Epoch 48 | train loss 0.1830 acc 0.9283 | val loss 0.2940 acc 0.8932\n",
      "Epoch 49 | train loss 0.1618 acc 0.9403 | val loss 0.3069 acc 0.8932\n",
      "Epoch 50 | train loss 0.1666 acc 0.9327 | val loss 0.3083 acc 0.8835\n",
      "Test set LOSS: 0.2400\n",
      "Test set ACCURACY: 0.9092\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.933747  0.880859  0.906533       512\n",
      "           1   0.887246  0.937500  0.911681       512\n",
      "\n",
      "    accuracy                       0.909180      1024\n",
      "   macro avg   0.910497  0.909180  0.909107      1024\n",
      "weighted avg   0.910497  0.909180  0.909107      1024\n",
      "\n",
      "traces.shape: (4096, 68)\n",
      "label.shape: (4096,)\n",
      "traces_train.shape: (2048, 68)\n",
      "traces_test.shape: (2048, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            516\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,152\n",
      "Trainable params: 17,152\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.3555 acc 0.3890 | val loss 1.3666 acc 0.2537\n",
      "Epoch 02 | train loss 1.2021 acc 0.5703 | val loss 1.1612 acc 0.6341\n",
      "Epoch 03 | train loss 0.9045 acc 0.6343 | val loss 0.7890 acc 0.6976\n",
      "Epoch 04 | train loss 0.7003 acc 0.7027 | val loss 0.6229 acc 0.7268\n",
      "Epoch 05 | train loss 0.6192 acc 0.7238 | val loss 0.5453 acc 0.7805\n",
      "Epoch 06 | train loss 0.5786 acc 0.7499 | val loss 0.5114 acc 0.7756\n",
      "Epoch 07 | train loss 0.5522 acc 0.7623 | val loss 0.4902 acc 0.8000\n",
      "Epoch 08 | train loss 0.5250 acc 0.7748 | val loss 0.4772 acc 0.8000\n",
      "Epoch 09 | train loss 0.5080 acc 0.7868 | val loss 0.4549 acc 0.8098\n",
      "Epoch 10 | train loss 0.4988 acc 0.7797 | val loss 0.4453 acc 0.7951\n",
      "Epoch 11 | train loss 0.4856 acc 0.7933 | val loss 0.4440 acc 0.8000\n",
      "Epoch 12 | train loss 0.4845 acc 0.7835 | val loss 0.4512 acc 0.8000\n",
      "Epoch 13 | train loss 0.4795 acc 0.7944 | val loss 0.4582 acc 0.8000\n",
      "Epoch 14 | train loss 0.4834 acc 0.7922 | val loss 0.4266 acc 0.8244\n",
      "Epoch 15 | train loss 0.4577 acc 0.8030 | val loss 0.4274 acc 0.8195\n",
      "Epoch 16 | train loss 0.4608 acc 0.8090 | val loss 0.4270 acc 0.8195\n",
      "Epoch 17 | train loss 0.4485 acc 0.8106 | val loss 0.4316 acc 0.8146\n",
      "Epoch 18 | train loss 0.4442 acc 0.8128 | val loss 0.4232 acc 0.8195\n",
      "Epoch 19 | train loss 0.4484 acc 0.8079 | val loss 0.4250 acc 0.8146\n",
      "Epoch 20 | train loss 0.4319 acc 0.8166 | val loss 0.4199 acc 0.8195\n",
      "Epoch 21 | train loss 0.4340 acc 0.8155 | val loss 0.4156 acc 0.8244\n",
      "Epoch 22 | train loss 0.4206 acc 0.8182 | val loss 0.4123 acc 0.8146\n",
      "Epoch 23 | train loss 0.4206 acc 0.8226 | val loss 0.4185 acc 0.8146\n",
      "Epoch 24 | train loss 0.4214 acc 0.8117 | val loss 0.4115 acc 0.8098\n",
      "Epoch 25 | train loss 0.4055 acc 0.8253 | val loss 0.4076 acc 0.8195\n",
      "Epoch 26 | train loss 0.4106 acc 0.8285 | val loss 0.4137 acc 0.8000\n",
      "Epoch 27 | train loss 0.4192 acc 0.8133 | val loss 0.4964 acc 0.7756\n",
      "Epoch 28 | train loss 0.4236 acc 0.8204 | val loss 0.4130 acc 0.8098\n",
      "Epoch 29 | train loss 0.4233 acc 0.8237 | val loss 0.4119 acc 0.7854\n",
      "Epoch 30 | train loss 0.4183 acc 0.8123 | val loss 0.4095 acc 0.8049\n",
      "Epoch 31 | train loss 0.4063 acc 0.8177 | val loss 0.4081 acc 0.8098\n",
      "Epoch 32 | train loss 0.4063 acc 0.8166 | val loss 0.4098 acc 0.8098\n",
      "Epoch 33 | train loss 0.3939 acc 0.8291 | val loss 0.4163 acc 0.8000\n",
      "Epoch 34 | train loss 0.3945 acc 0.8291 | val loss 0.4399 acc 0.8244\n",
      "Epoch 35 | train loss 0.4073 acc 0.8139 | val loss 0.4099 acc 0.7805\n",
      "Epoch 36 | train loss 0.3865 acc 0.8345 | val loss 0.4147 acc 0.8195\n",
      "Epoch 37 | train loss 0.3803 acc 0.8318 | val loss 0.4054 acc 0.8195\n",
      "Epoch 38 | train loss 0.3622 acc 0.8470 | val loss 0.4146 acc 0.8098\n",
      "Epoch 39 | train loss 0.3814 acc 0.8383 | val loss 0.4047 acc 0.8146\n",
      "Epoch 40 | train loss 0.3838 acc 0.8448 | val loss 0.4052 acc 0.7951\n",
      "Epoch 41 | train loss 0.3782 acc 0.8351 | val loss 0.4049 acc 0.8098\n",
      "Epoch 42 | train loss 0.3661 acc 0.8410 | val loss 0.4056 acc 0.8293\n",
      "Epoch 43 | train loss 0.3763 acc 0.8361 | val loss 0.3981 acc 0.8098\n",
      "Epoch 44 | train loss 0.3768 acc 0.8432 | val loss 0.4070 acc 0.7951\n",
      "Epoch 45 | train loss 0.3734 acc 0.8421 | val loss 0.4412 acc 0.8049\n",
      "Epoch 46 | train loss 0.3763 acc 0.8388 | val loss 0.4121 acc 0.8146\n",
      "Epoch 47 | train loss 0.3768 acc 0.8345 | val loss 0.4055 acc 0.8244\n",
      "Epoch 48 | train loss 0.3627 acc 0.8432 | val loss 0.3982 acc 0.8049\n",
      "Epoch 49 | train loss 0.3622 acc 0.8530 | val loss 0.4102 acc 0.8146\n",
      "Epoch 50 | train loss 0.3650 acc 0.8388 | val loss 0.4078 acc 0.8244\n",
      "Test set LOSS: 0.3962\n",
      "Test set ACCURACY: 0.8354\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.782847  0.837891  0.809434       512\n",
      "           1   0.896624  0.830078  0.862069       512\n",
      "           2   0.941176  0.968750  0.954764       512\n",
      "           3   0.723447  0.705078  0.714144       512\n",
      "\n",
      "    accuracy                       0.835449      2048\n",
      "   macro avg   0.836024  0.835449  0.835103      2048\n",
      "weighted avg   0.836024  0.835449  0.835103      2048\n",
      "\n",
      "traces.shape: (4096, 68)\n",
      "label.shape: (4096,)\n",
      "traces_train.shape: (2048, 68)\n",
      "traces_test.shape: (2048, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            516\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,152\n",
      "Trainable params: 17,152\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.3570 acc 0.3668 | val loss 1.3598 acc 0.2488\n",
      "Epoch 02 | train loss 1.1726 acc 0.6012 | val loss 1.0977 acc 0.7902\n",
      "Epoch 03 | train loss 0.8312 acc 0.7396 | val loss 0.6855 acc 0.7610\n",
      "Epoch 04 | train loss 0.6122 acc 0.7840 | val loss 0.5085 acc 0.7951\n",
      "Epoch 05 | train loss 0.5091 acc 0.8085 | val loss 0.4617 acc 0.8000\n",
      "Epoch 06 | train loss 0.4545 acc 0.8334 | val loss 0.4296 acc 0.8195\n",
      "Epoch 07 | train loss 0.4316 acc 0.8367 | val loss 0.3887 acc 0.8390\n",
      "Epoch 08 | train loss 0.4022 acc 0.8497 | val loss 0.3581 acc 0.8537\n",
      "Epoch 09 | train loss 0.3882 acc 0.8497 | val loss 0.3933 acc 0.8098\n",
      "Epoch 10 | train loss 0.3744 acc 0.8589 | val loss 0.3618 acc 0.8439\n",
      "Epoch 11 | train loss 0.3614 acc 0.8600 | val loss 0.3436 acc 0.8488\n",
      "Epoch 12 | train loss 0.3574 acc 0.8568 | val loss 0.3165 acc 0.8732\n",
      "Epoch 13 | train loss 0.3572 acc 0.8654 | val loss 0.3228 acc 0.8976\n",
      "Epoch 14 | train loss 0.3504 acc 0.8676 | val loss 0.3262 acc 0.8829\n",
      "Epoch 15 | train loss 0.3498 acc 0.8638 | val loss 0.3248 acc 0.8732\n",
      "Epoch 16 | train loss 0.3176 acc 0.8714 | val loss 0.2992 acc 0.8829\n",
      "Epoch 17 | train loss 0.3210 acc 0.8774 | val loss 0.3024 acc 0.8634\n",
      "Epoch 18 | train loss 0.3222 acc 0.8736 | val loss 0.3075 acc 0.8732\n",
      "Epoch 19 | train loss 0.3240 acc 0.8741 | val loss 0.3106 acc 0.8537\n",
      "Epoch 20 | train loss 0.3158 acc 0.8736 | val loss 0.2884 acc 0.8780\n",
      "Epoch 21 | train loss 0.3077 acc 0.8812 | val loss 0.2835 acc 0.8829\n",
      "Epoch 22 | train loss 0.3081 acc 0.8790 | val loss 0.2857 acc 0.8537\n",
      "Epoch 23 | train loss 0.3027 acc 0.8871 | val loss 0.2657 acc 0.8927\n",
      "Epoch 24 | train loss 0.2903 acc 0.8812 | val loss 0.2889 acc 0.8780\n",
      "Epoch 25 | train loss 0.2934 acc 0.8828 | val loss 0.2909 acc 0.8634\n",
      "Epoch 26 | train loss 0.2915 acc 0.8866 | val loss 0.2844 acc 0.8634\n",
      "Epoch 27 | train loss 0.2904 acc 0.8817 | val loss 0.2608 acc 0.8976\n",
      "Epoch 28 | train loss 0.2875 acc 0.8882 | val loss 0.2729 acc 0.8634\n",
      "Epoch 29 | train loss 0.2887 acc 0.8904 | val loss 0.2819 acc 0.8537\n",
      "Epoch 30 | train loss 0.2819 acc 0.8942 | val loss 0.2598 acc 0.8780\n",
      "Epoch 31 | train loss 0.2862 acc 0.8893 | val loss 0.2655 acc 0.8732\n",
      "Epoch 32 | train loss 0.2740 acc 0.8904 | val loss 0.2767 acc 0.8585\n",
      "Epoch 33 | train loss 0.2829 acc 0.8942 | val loss 0.2566 acc 0.8829\n",
      "Epoch 34 | train loss 0.2733 acc 0.8861 | val loss 0.2478 acc 0.8927\n",
      "Epoch 35 | train loss 0.2727 acc 0.8953 | val loss 0.2595 acc 0.8780\n",
      "Epoch 36 | train loss 0.2731 acc 0.8866 | val loss 0.2466 acc 0.8878\n",
      "Epoch 37 | train loss 0.2640 acc 0.8909 | val loss 0.2557 acc 0.8585\n",
      "Epoch 38 | train loss 0.2606 acc 0.8920 | val loss 0.2384 acc 0.9122\n",
      "Epoch 39 | train loss 0.2632 acc 0.8958 | val loss 0.2442 acc 0.8878\n",
      "Epoch 40 | train loss 0.2583 acc 0.9012 | val loss 0.2732 acc 0.8780\n",
      "Epoch 41 | train loss 0.2648 acc 0.9012 | val loss 0.2634 acc 0.8537\n",
      "Epoch 42 | train loss 0.2662 acc 0.8909 | val loss 0.2359 acc 0.8976\n",
      "Epoch 43 | train loss 0.2671 acc 0.8953 | val loss 0.2432 acc 0.9024\n",
      "Epoch 44 | train loss 0.2493 acc 0.9034 | val loss 0.2703 acc 0.8683\n",
      "Epoch 45 | train loss 0.2513 acc 0.9029 | val loss 0.2465 acc 0.9073\n",
      "Epoch 46 | train loss 0.2491 acc 0.9012 | val loss 0.2237 acc 0.9073\n",
      "Epoch 47 | train loss 0.2477 acc 0.9045 | val loss 0.2191 acc 0.9122\n",
      "Epoch 48 | train loss 0.2452 acc 0.9050 | val loss 0.2240 acc 0.9024\n",
      "Epoch 49 | train loss 0.2469 acc 0.8996 | val loss 0.2332 acc 0.8927\n",
      "Epoch 50 | train loss 0.2495 acc 0.9018 | val loss 0.2248 acc 0.8976\n",
      "Test set LOSS: 0.3050\n",
      "Test set ACCURACY: 0.8896\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.896761  0.865234  0.880716       512\n",
      "           1   0.925620  0.875000  0.899598       512\n",
      "           2   0.823308  0.855469  0.839080       512\n",
      "           3   0.916357  0.962891  0.939048       512\n",
      "\n",
      "    accuracy                       0.889648      2048\n",
      "   macro avg   0.890512  0.889648  0.889611      2048\n",
      "weighted avg   0.890512  0.889648  0.889611      2048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_shift, scaler_shift = \\\n",
    "#     build_fpr_templates_ml(traces_mul_dict, start_end_indices['mul_shift'],\n",
    "#                            test_split, class_label_map_shift, 100)\n",
    "model_4_5, scaler_4_5, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_4_5, 100)\n",
    "model_6_7, scaler_6_7, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_6_7, 100)\n",
    "model_8_11, scaler_8_11, y_prob  = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_8_11, 200)\n",
    "model_12_15, scaler_12_15, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_12_15, 100)\n",
    "# model_16_22, scaler_16_22, y_prob = \\\n",
    "#     build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "#                            test_split, class_label_map_16_22, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e1582c-2138-4525-a01c-eed58510acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class_map = {\n",
    "    0: [0],\n",
    "    1: [1],\n",
    "    2: [-1],\n",
    "    3: [2, 3],\n",
    "    4: [-2, -3],\n",
    "    5: [4, 5, 6, 7],\n",
    "    6: [-4, -5, -6, -7],\n",
    "    7: list(range(8, 16)),\n",
    "    8: list(range(-15, -7)),\n",
    "    # 9: list(range(16, 32)),\n",
    "    # 10: list(range(-31, -15)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0963618-bc60-41c2-9eca-5d0df19050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class_abs_map = {\n",
    "    0: [0],\n",
    "    1: [1],\n",
    "    2: [2, 3],\n",
    "    3: [4, 5, 6, 7],\n",
    "    4: list(range(8, 16)),\n",
    "    # 5: list(range(16, 32)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d54564-60f8-4a94-b91e-68560225c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_choose_value(prob_dict):\n",
    "    mle_tmp_f_dict={}\n",
    "    for k, prob_array in prob_dict.items():\n",
    "        log_prob_array = np.log(prob_array)\n",
    "        col_sum = np.sum(log_prob_array, axis=0)\n",
    "        max_index = np.argmax(col_sum)\n",
    "        mle_tmp_f_dict[k]=max_index\n",
    "    return mle_tmp_f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766cb79d-07ef-4067-b195-89d58b9436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sign(guess_attack_scaled,falcon_n):\n",
    "   sign_dict = {}\n",
    "   for i in range(falcon_n):\n",
    "        ele = guess_attack_scaled[i]\n",
    "        if new_class_map[ele][0] == 0:\n",
    "            sign_dict[i] = 0\n",
    "        elif new_class_map[ele][0] > 0:\n",
    "            sign_dict[i] = 1\n",
    "        elif new_class_map[ele][0] < 0:\n",
    "            sign_dict[i] = -1\n",
    "   return sign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea47e535-a93c-47cc-80d9-8ce7587966ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_abs_table_index(guess_attack_scaled, falcon_n):\n",
    "    abs_dict = {}\n",
    "    for i in range(falcon_n):\n",
    "        ele = guess_attack_scaled[i]\n",
    "        if ele == 0:\n",
    "            abs_dict[i] = 0\n",
    "        elif ele == 1 or ele == 2:\n",
    "            abs_dict[i] = 1\n",
    "        elif ele == 3 or ele == 4:\n",
    "            abs_dict[i] = 2\n",
    "        elif ele == 5 or ele == 6:\n",
    "            abs_dict[i] = 3\n",
    "        elif ele == 7 or ele == 8:\n",
    "            abs_dict[i] = 4\n",
    "        elif ele == 9 or ele == 10:\n",
    "            abs_dict[i] = 5   \n",
    "    return abs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8db0fb0-f934-4c3a-b148-03ca66ba2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reorder_traces(A):\n",
    "    \"\"\"\n",
    "   Adjust the row order of A from [128,384,128,384,...] to [128,128,129,129,...,511,511]\n",
    "    \"\"\"\n",
    "    # 偶数行对应小索引 (128..255)\n",
    "    small = A[0::2, :]\n",
    "    # 奇数行对应大索引 (384..511)\n",
    "    large = A[1::2, :]\n",
    "    \n",
    "    # 拼接：先小索引两条，再大索引两条\n",
    "    A_new = np.vstack([small, large])\n",
    "    return A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab09a582-758f-460b-9398-f70765ca2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffset_label(co_labels, target_filtered, set0):\n",
    "    label_diffset = []\n",
    "    for c in target_filtered:\n",
    "        if co_labels[c] in set0 or -co_labels[c] in set0:\n",
    "            label_diffset.append(0)\n",
    "        else: \n",
    "            label_diffset.append(1)\n",
    "    label_diffset = np.array(label_diffset)\n",
    "    return label_diffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce7a605d-0894-4100-82ce-a415a8ee18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_rows_by_labels(M, labels, subset):\n",
    "    \"\"\"\n",
    "    Select rows from a 2D array M, with row labels provided by `labels`.\n",
    "    \n",
    "    Parameters:  \n",
    "    M: numpy.array, shape = (n, m)\n",
    "    labels: list or array, length = n, the labels for each row in M\n",
    "    subset: list or set, the desired subset of labels\n",
    "    \n",
    "    Returns:\n",
    "    N: numpy.array, shape = (len(subset), m),\n",
    "    The row order is the same as in the subset.\n",
    "    \"\"\"\n",
    "    labels = np.array(labels)\n",
    "    subset = list(subset)  # 保留输入顺序\n",
    "    idx = [np.where(labels == s)[0][0] for s in subset]\n",
    "    N = M[idx, :]\n",
    "    return N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d8d24e4-3804-484c-8428-01eb3ff10ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fill_dict(to_fill_dict, prob_dict):\n",
    "    for kk, prob_list in prob_dict.items():\n",
    "        if kk not in to_fill_dict:\n",
    "            to_fill_dict[kk] = prob_list\n",
    "        else:\n",
    "            to_fill_dict[kk] = np.vstack([to_fill_dict[kk], prob_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d5fc2a-fb26-4ace-8058-29438b5a2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_process(fpr_diffset_mle_prob_list):\n",
    "    fpr_diffset_mle_prob_list = dict(fpr_diffset_mle_prob_list)\n",
    "    fpr_diffset_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_diffset_mle_prob_list.items()}\n",
    "    return fpr_diffset_mle_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5e41b74-de52-4a34-a321-3228a71e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cal_per_mul_prob(sets_dict, model_p,scaler_p,list_range):\n",
    "    \"\"\"\n",
    "    Concatenates, computes, and splits the dictionary `{key: (2, n) array}` back into a dictionary.\n",
    "    Parameters:\n",
    "    `data_dict`: dict[int, np.ndarray]\n",
    "    Each value is a NumPy array of (2, n) values.\n",
    "    `func`: callable\n",
    "    A function that takes (2*m, n) as input and outputs (2*m, k) values.\n",
    "    \n",
    "    Returns:\n",
    "    `new_dict`: dict[int, np.ndarray]\n",
    "     Each value is a NumPy array of (2, k) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Concatenate the keys in order to form a large matrix.\n",
    "    keys = list(sets_dict.keys())\n",
    "    big_array = np.vstack([sets_dict[k] for k in keys])\n",
    "    # print(\"dimension\")\n",
    "    # print(big_array.shape)\n",
    "    \n",
    "    # 2. calculate\n",
    "    # tmp_labels = np.repeat(labels,2)\n",
    "    result = classifier_inference(big_array, list_range, model_p, scaler_p)\n",
    "    \n",
    "    # 3. Split back into dictionary\n",
    "    new_dict = {}\n",
    "    for idx, key in enumerate(keys):\n",
    "        start = idx * 2\n",
    "        end = start + 2\n",
    "        new_dict[key] = result[start:end, :]\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04649cc9-bbc8-4f8c-9ecd-1c28003e7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 1000 keys in the database; I only collected the attack curves for the first 100 keys.\n",
    "filename_f = \"./data_k_828/Falcon_f_1024_1000.csv\"\n",
    "filename_g = \"./data_k_828/Falcon_g_1024_1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cee89a96-ecec-4e26-a4d8-6d5c1bb0506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The passed-in `guess_part_list` consists of dictionaries for guessing categories {4,5} and {6,7}, respectively.\n",
    "def fill_guess_dict(guess_part_list,f_final_set_p,f_sign_dict_p):\n",
    "    for idx_guess, guess_p in enumerate(guess_part_list):\n",
    "        for key_, guess_v in guess_p.items():\n",
    "            if idx_guess == 0: \n",
    "                f_final_set_p[key_]={guess_v+4} \n",
    "            if idx_guess == 1:\n",
    "                f_final_set_p[key_]={guess_v+6} \n",
    "    sorted_f_final_set_p = dict(sorted(f_final_set_p.items()))\n",
    "    f_guess_dict_p={}\n",
    "    for key, val in f_sign_dict_p.items():\n",
    "        if val>=0:\n",
    "            f_guess_dict_p[key] = sorted_f_final_set_p[key]\n",
    "        else:\n",
    "            f_guess_dict_p[key] = {-x for x in sorted_f_final_set_p[key]}\n",
    "    return f_guess_dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c30965e-ee7b-4fcd-bbd1-aef62525565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def attack_key_repeat(attack_scaled, attack_shift, attack_mul, mle_N):\n",
    "    #########First, process the data from fpr_scaled.\n",
    "    attack_traces_scaled = attack_scaled.reshape(-1,484)\n",
    "    attack_traces_scaled_lpf = trace_butter_lpf(attack_traces_scaled, sample_freq, cutoff_freq)\n",
    "    attack_traces_scaled_lpf = attack_traces_scaled_lpf.reshape(-1,1024,484)\n",
    "    \n",
    "    fpr_scaled_mle_prob_list = defaultdict(list)\n",
    "    for i in range(mle_N):\n",
    "        prob_fpr_scaled = classifier_inference(attack_traces_scaled_lpf[i,:,:], list(range(484)), model_scaled, scaler_scaled)\n",
    "        for j in range(1024):\n",
    "            fpr_scaled_mle_prob_list[j].append(prob_fpr_scaled[j])\n",
    "   \n",
    "    fpr_scaled_mle_prob_list = dict(fpr_scaled_mle_prob_list)\n",
    "    fpr_scaled_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_scaled_mle_prob_list.items()}\n",
    "    scaled_dict = mle_choose_value(fpr_scaled_mle_prob_list)\n",
    "\n",
    "    #Record information after attacking fpr_scaled\n",
    "    sign_dict = record_sign(scaled_dict,1024)\n",
    "    abs_dict = record_abs_table_index(scaled_dict,1024)\n",
    "    # print(abs_dict)\n",
    "\n",
    "    ######### Process the trace data corresponding to the normalization procedure in fpr_mul\n",
    "    #f_128~f_255; f_384~f_511 will involve multiplication operations. We need to analyze these coefficients, \n",
    "    #identifying which are already 0, -1, or 1, and which require further differentiation.\n",
    "    check_keys = list(range(256, 512)) + list(range(768, 1024))\n",
    "    keys_012 = [k for k in check_keys if scaled_dict[k] in (0, 1, 2)]\n",
    "    # keys_other = [k for k in check_keys if scaled_dict[k] not in (0, 1, 2)]\n",
    "  \n",
    "    # At this point, the `mle_N` observations should be utilized.\n",
    "    set0 = {2, 4, 5, 8, 9, 10, 11}\n",
    "    set1 = {3, 6, 7, 12, 13, 14, 15}\n",
    "    target_order = list(range(256, 512)) + list(range(768, 1024))\n",
    "    target_filtered = [c for c in target_order if c not in keys_012]\n",
    "    keep_rows = [i for i, c in enumerate(target_order) if c not in keys_012]\n",
    "    \n",
    "    fpr_shift_mle_prob_list = defaultdict(list)\n",
    "    \n",
    "    for i in range(mle_N):\n",
    "        attack_traces_shift = reorder_traces(attack_shift[i,:,:]) \n",
    "        attack_traces_shift_lpf = trace_butter_lpf(attack_traces_shift, sample_freq, cutoff_freq)\n",
    "        attack_traces_shift_lpf_1 = attack_traces_shift_lpf[0::2] \n",
    "        attack_traces_shift_lpf_2 = attack_traces_shift_lpf[1::2] \n",
    "        \n",
    "       # Continuing with the processing of attack_traces_shift_lpf, since the coefficient indices in keys_012 are already determined, \n",
    "        # no further processing is needed; therefore, this part of the data is filtered out.\n",
    "        attack_traces_shift_filtered_1 = attack_traces_shift_lpf_1[keep_rows, :]\n",
    "        attack_traces_shift_filtered_2 = attack_traces_shift_lpf_2[keep_rows, :]\n",
    "\n",
    "        prob_fpr_shift_1 = classifier_inference(attack_traces_shift_filtered_1, list(range(88)), model_shift, scaler_shift)\n",
    "        # print(prob_fpr_diffset_1)\n",
    "        prob_fpr_shift_2 = classifier_inference(attack_traces_shift_filtered_2, list(range(88)), model_shift, scaler_shift)\n",
    "        # print(prob_fpr_diffset_2)\n",
    "        \n",
    "        for j in range(len(target_filtered)):\n",
    "            fpr_shift_mle_prob_list[target_filtered[j]].append(prob_fpr_shift_1[j])\n",
    "            fpr_shift_mle_prob_list[target_filtered[j]].append(prob_fpr_shift_2[j])\n",
    "    \n",
    "    fpr_shift_mle_prob_list = dict(fpr_shift_mle_prob_list)\n",
    "    fpr_shift_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_shift_mle_prob_list.items()}\n",
    "    shift_dict = mle_choose_value(fpr_shift_mle_prob_list)#mle得到标签值\n",
    "    # print(shift_dict)\n",
    "\n",
    "     ######Based on the attack results of fpr_scaled and the normalization procedure in fpr_mul, \n",
    "    # the initial range of values for the partitioning coefficients is determined.\n",
    "    final_set = {}\n",
    "    sure_set=[]\n",
    "    insure_set=[]\n",
    "    for key, value in abs_dict.items():\n",
    "        if value == 0 or value == 1:\n",
    "            sure_set.append(key)\n",
    "            final_set[key]=set(new_class_abs_map[value])\n",
    "        else:\n",
    "            insure_set.append(key)\n",
    "\n",
    "    for insure_idx in insure_set:\n",
    "        if insure_idx in target_filtered:\n",
    "            # label_choose = guess_attack_diffset[target_filtered_f.index(insure_idx)]\n",
    "            label_choose = shift_dict[insure_idx]\n",
    "            if label_choose == 0:\n",
    "                tmp_set = set0 & set(new_class_abs_map[abs_dict[insure_idx]])\n",
    "                final_set[insure_idx]=tmp_set\n",
    "            else:\n",
    "                tmp_set = set1 & set(new_class_abs_map[abs_dict[insure_idx]])\n",
    "                final_set[insure_idx]=tmp_set\n",
    "\n",
    "    no_mul_w = list(range(0, 256)) + list(range(512, 768))\n",
    "    no_mul_w_insure = [d for d in no_mul_w if d not in sure_set]\n",
    "    for d in no_mul_w_insure:\n",
    "        final_set[d] = set(new_class_abs_map[abs_dict[d]])\n",
    "\n",
    "    fpr_mul_mle_prob_list_45 = defaultdict(list)\n",
    "    fpr_mul_mle_prob_list_67 = defaultdict(list)\n",
    "    mul_idx_list=[]\n",
    "    mul_w = list(range(256, 512)) + list(range(768, 1024))\n",
    "    for d in mul_w:\n",
    "        if len(final_set[d]) != 1:\n",
    "            mul_idx_list.append(d)\n",
    "\n",
    "\n",
    "    for i in range(mle_N):\n",
    "        \n",
    "        attack_traces_mul = reorder_traces(attack_mul[i,:,:])\n",
    "        attack_traces_mul_1 = attack_traces_mul[0::2] \n",
    "        attack_traces_mul_2 = attack_traces_mul[1::2] \n",
    "        attack_traces_mul_lpf_1 = trace_butter_lpf(attack_traces_mul_1, sample_freq, cutoff_freq)\n",
    "        attack_traces_mul_lpf_2 = trace_butter_lpf(attack_traces_mul_2, sample_freq, cutoff_freq)\n",
    "        attack_traces_mul_lpf_1 = select_rows_by_labels(attack_traces_mul_lpf_1, mul_w, mul_idx_list)\n",
    "        attack_traces_mul_lpf_2 = select_rows_by_labels(attack_traces_mul_lpf_2, mul_w, mul_idx_list)\n",
    "        \n",
    "        \n",
    "        sets_4_5 = defaultdict(list)\n",
    "        sets_6_7 = defaultdict(list)  \n",
    "       \n",
    "        for i in range(len(mul_idx_list)):\n",
    "            if final_set[mul_idx_list[i]] == {4,5}:\n",
    "                sets_4_5[mul_idx_list[i]].append(attack_traces_mul_lpf_1[i])\n",
    "                sets_4_5[mul_idx_list[i]].append(attack_traces_mul_lpf_2[i])\n",
    "                # labels_4_5.append(abs(mul_labels[mul_idx_list[i]])-4)\n",
    "            elif final_set[mul_idx_list[i]] == {6,7}:\n",
    "                sets_6_7[mul_idx_list[i]].append(attack_traces_mul_lpf_1[i])\n",
    "                sets_6_7[mul_idx_list[i]].append(attack_traces_mul_lpf_2[i])\n",
    "               \n",
    "\n",
    "        sets_4_5 = dict_process(sets_4_5)\n",
    "        sets_6_7 = dict_process(sets_6_7)\n",
    "        # labels_4_5 = np.array(labels_4_5)\n",
    "   \n",
    "        prob_fpr_mul_4_5 = cal_per_mul_prob(sets_4_5,model_4_5,scaler_4_5,list(range(68)))\n",
    "        prob_fpr_mul_6_7 = cal_per_mul_prob(sets_6_7,model_6_7,scaler_6_7,list(range(68))) \n",
    "        # print(prob_fpr_mul_4_5)\n",
    "            \n",
    "        fill_dict(fpr_mul_mle_prob_list_45, prob_fpr_mul_4_5)\n",
    "        fill_dict(fpr_mul_mle_prob_list_67, prob_fpr_mul_6_7)\n",
    "       \n",
    "    fpr_mul_mle_prob_list_45 = dict_process(fpr_mul_mle_prob_list_45)\n",
    "    fpr_mul_mle_prob_list_67 = dict_process(fpr_mul_mle_prob_list_67)\n",
    "    # print(fpr_mul_mle_prob_list_45)\n",
    "    \n",
    "    guess_45 = mle_choose_value(fpr_mul_mle_prob_list_45)\n",
    "    guess_67 = mle_choose_value(fpr_mul_mle_prob_list_67)\n",
    "    # print(guess_45)\n",
    "\n",
    "    part_guess_list=[guess_45, guess_67]\n",
    "    final_guess_dict = fill_guess_dict(part_guess_list,final_set,sign_dict)\n",
    "            \n",
    "    return final_guess_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3ed5643-33fe-4ab2-8676-8c14392316b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge f and g into f||g, first check if the number of currently \"determined\" coefficients (with only one candidate value) is greater than or equal to 1024.\n",
    "# If less than 1024, fail directly.\n",
    "# If equal to 1024, check if these 1024 are correct.\n",
    "# If greater than 1024, set it as num_guess, and randomly select 1024 from num_guess each time, continuously checking for correctness. If none of these attempts succeed, then it's a failure.\n",
    "\n",
    "#The extraction principle is to prioritize keeping those definitely 0 or ±1, followed by ±2 or ±3, then ±4 or ±5, and finally ±6 or ±7.\n",
    "\n",
    "#Return values: -1: fewer than 1024 confirmed cases; -2: exactly 1024 confirmed cases, but with errors; -3: len(candidate) < 1024 - need;\n",
    "#-4: multiple attempts failed. 1: exactly 1024 successful; 2: successful after random sampling; 3: successful when the value reaches {2, 3}.\n",
    "\n",
    "def check_key_right(f_guess_dict_p, g_guess_dict_p, filename_f_p, filename_g_p, kk_idx, attempts):\n",
    " \n",
    "    import csv, random\n",
    "\n",
    "    f_determined = {k: list(v)[0] for k, v in f_guess_dict_p.items() if len(v) == 1}\n",
    "    g_determined = {k: list(v)[0] for k, v in g_guess_dict_p.items() if len(v) == 1}\n",
    "    total_determined = len(f_determined) + len(g_determined)\n",
    "\n",
    "    if total_determined < 1024:\n",
    "        return -1\n",
    "\n",
    "    def read_row(csv_path, row_idx):\n",
    "        with open(csv_path, newline=\"\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for j, row in enumerate(reader):\n",
    "                if j == row_idx:\n",
    "                    return list(map(int, row))\n",
    "        raise ValueError(f\"Row {row_idx} not found in {csv_path}\")\n",
    "\n",
    "    f_row = read_row(filename_f_p, kk_idx)\n",
    "    g_row = read_row(filename_g_p, kk_idx)\n",
    "\n",
    "    if total_determined == 1024:\n",
    "        for k, v in f_determined.items():\n",
    "            if f_row[k] != v:\n",
    "                return -2\n",
    "        for k, v in g_determined.items():\n",
    "            if g_row[k] != v:\n",
    "                return -2\n",
    "        return 1\n",
    "\n",
    "    \n",
    "    #check the value at a specific location within {0, ±1, ±2, ±3}. \n",
    "    # This step is equivalent to the O0 level experiment.\n",
    "    \n",
    "    f_dict_p = {}\n",
    "    g_dict_p = {}\n",
    "  \n",
    "    # special_values = {0, 1, -1, 2, -2, 3, -3, 4, -4, 5, -5}\n",
    "    special_values = {0, 1, -1, 2, -2, 3, -3}\n",
    "    priority_positions = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v in special_values:\n",
    "            priority_positions.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v in special_values:\n",
    "            priority_positions.append((\"g\", k, v))\n",
    "\n",
    "    if len(priority_positions) >= 1024:\n",
    "        print(\"jinru\")\n",
    "        ok = True\n",
    "        for src, k, v in priority_positions:\n",
    "            if src == \"f\":\n",
    "                if f_row[k] != v:\n",
    "                    # print(\"f\")\n",
    "                    # print(k)\n",
    "                    # print(f_row[k])\n",
    "                    # print(v)\n",
    "                    ok = False\n",
    "                    break\n",
    "            else:  # \"g\"\n",
    "                if g_row[k] != v:\n",
    "                    # print(\"g\")\n",
    "                    # print(k)\n",
    "                    ok = False\n",
    "                    break\n",
    "        if ok:\n",
    "            for source, k, v in priority_positions:\n",
    "                if source == \"f\":\n",
    "                    f_dict_p[k] = v\n",
    "                elif source == \"g\":\n",
    "                    g_dict_p[k] = v\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon1024/f_{kk_idx}_guess.pkl\", \"wb\") as f:\n",
    "                pickle.dump(f_dict_p, f)\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon1024/g_{kk_idx}_guess.pkl\", \"wb\") as g:\n",
    "                pickle.dump(g_dict_p, g)        \n",
    "            return 3\n",
    "    \n",
    "    # --- Step3: total_determined > 512 ---\n",
    "    special_positions = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v in (0, 1, -1):\n",
    "            special_positions.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v in (0, 1, -1):\n",
    "            special_positions.append((\"g\", k, v))\n",
    "\n",
    "    m = len(special_positions)\n",
    "\n",
    "    candidates = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v not in (0, 1, -1):\n",
    "            candidates.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v not in (0, 1, -1):\n",
    "            candidates.append((\"g\", k, v))\n",
    "\n",
    "    need = 1024 - m\n",
    "    print(f\"need is {need}\")\n",
    "    print(f\"len of candidates is {len(candidates)}\")\n",
    "    if need < 0 or len(candidates) < need:\n",
    "        return -3\n",
    "\n",
    "    # attempts = 10^6  # can adjust\n",
    "    for _ in range(attempts):\n",
    "        sample = random.sample(candidates, need)\n",
    "        positions = special_positions + sample\n",
    "\n",
    "        ok = True\n",
    "        for src, k, v in positions:\n",
    "            if src == \"f\":\n",
    "                if f_row[k] != v:\n",
    "                    ok = False\n",
    "                    break\n",
    "            else:  # src == \"g\"\n",
    "                if g_row[k] != v:\n",
    "                    ok = False\n",
    "                    break\n",
    "        if ok:\n",
    "            for source, k, v in positions:\n",
    "                if source == \"f\":\n",
    "                    f_dict_p[k] = v\n",
    "                elif source == \"g\":\n",
    "                    g_dict_p[k] = v\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon1024/f_{kk_idx}_guess.pkl\", \"wb\") as f:\n",
    "                pickle.dump(f_dict_p, f)\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon1024/g_{kk_idx}_guess.pkl\", \"wb\") as g:\n",
    "                pickle.dump(g_dict_p, g)       \n",
    "            return 2\n",
    "\n",
    "    return -4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efb2c7e0-79ce-4ac3-95c1-1d2df907828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1024, 484)\n"
     ]
    }
   ],
   "source": [
    "aaaa = np.load(f\"./data-O3-828/falcon1024_attack/fpr_scaled/0th_f_50traces.npy\")\n",
    "print(aaaa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7db3371e-3cf1-4c23-9356-9346b21f364d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jinru\n",
      "the 0th key guess's state is 3\n",
      "jinru\n",
      "need is 212\n",
      "len of candidates is 614\n",
      "the 1th key guess's state is 2\n",
      "jinru\n",
      "need is 205\n",
      "len of candidates is 606\n",
      "the 2th key guess's state is -4\n",
      "jinru\n",
      "the 3th key guess's state is 3\n",
      "jinru\n",
      "the 4th key guess's state is 3\n",
      "jinru\n",
      "the 5th key guess's state is 3\n",
      "jinru\n",
      "the 6th key guess's state is 3\n",
      "jinru\n",
      "the 7th key guess's state is 3\n",
      "jinru\n",
      "the 8th key guess's state is 3\n",
      "jinru\n",
      "need is 186\n",
      "len of candidates is 577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     23\u001b[39m g_guess_dict = attack_key_repeat(attack_traces_scaled_g,attack_traces_diffset_g,attack_traces_multi_g,mle_N)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#把f和g合并为f||g，首先看目前“确定”的系数(只有一个候选值)的个数是否大于等于512\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#如果小于512，直接失败\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#如果等于512，判断这512个是否正确  \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#返回值： -1：确定的小于512， -2：正好512个确定的是正确的， -3：len(candidate)<512-need -4:重复多次没有成功  1：512个正好成功  2：random sample后成功\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#3：取值到{4，5}成功\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m re = \u001b[43mcheck_key_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_guess_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_guess_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_random_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_th\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mth key guess\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms state is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m re>\u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mcheck_key_right\u001b[39m\u001b[34m(f_guess_dict_p, g_guess_dict_p, filename_f_p, filename_g_p, kk_idx, attempts)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# --- 随机尝试若干次，避免死循环 ---\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# attempts = 50  # 可调节\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(attempts):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     sample = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     positions = special_positions + sample\n\u001b[32m    128\u001b[39m     ok = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\software\\ChipWhisperer6.0\\winpy\\python\\Lib\\random.py:440\u001b[39m, in \u001b[36mRandom.sample\u001b[39m\u001b[34m(self, population, k, counts)\u001b[39m\n\u001b[32m    438\u001b[39m pool = \u001b[38;5;28mlist\u001b[39m(population)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     j = \u001b[43mrandbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     result[i] = pool[j]\n\u001b[32m    442\u001b[39m     pool[j] = pool[n - i - \u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# move non-selected item into vacancy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\software\\ChipWhisperer6.0\\winpy\\python\\Lib\\random.py:242\u001b[39m, in \u001b[36mRandom._randbelow_with_getrandbits\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    239\u001b[39m             \u001b[38;5;28mcls\u001b[39m._randbelow = \u001b[38;5;28mcls\u001b[39m._randbelow_without_getrandbits\n\u001b[32m    240\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_randbelow_with_getrandbits\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[32m    243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mReturn a random int in the range [0,n).  Defined for n > 0.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m     getrandbits = \u001b[38;5;28mself\u001b[39m.getrandbits\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "random.seed(2025)\n",
    "mle_N = 10\n",
    "try_random_num = 100000\n",
    "succ_num_count=0\n",
    "num_attack_key=100\n",
    "key_key_idx_list=[]\n",
    "error_list=[]\n",
    "# aaa_list = [0,10,22,25,30,72,80,90]\n",
    "# for key_th in range(num_attack_key):\n",
    "for key_th in range(num_attack_key):\n",
    "    # the attack trace segments corresponding to the fpr_scaled, normalization procedure and mantissa multiplication in fpr_mul for the f key\n",
    "    attack_traces_scaled_f = np.load(f\"./data-O3-828/falcon1024_attack/fpr_scaled/{key_th}th_f_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_diffset_f = np.load(f\"./data-O3-828/falcon1024_attack/fpr_mul/{key_th}th_f_shift_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_multi_f = np.load(f\"./data-O3-828/falcon1024_attack/fpr_mul/{key_th}th_f_multiply_50traces.npy\")[:mle_N,:,:]\n",
    "    # the attack trace segments corresponding to the fpr_scaled, normalization procedure and mantissa multiplication in fpr_mul for the g key\n",
    "    attack_traces_scaled_g = np.load(f\"./data-O3-828/falcon1024_attack/fpr_scaled/{key_th}th_g_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_diffset_g = np.load(f\"./data-O3-828/falcon1024_attack/fpr_mul/{key_th}th_g_shift_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_multi_g = np.load(f\"./data-O3-828/falcon1024_attack/fpr_mul/{key_th}th_g_multiply_50traces.npy\")[:mle_N,:,:]\n",
    "    \n",
    "    f_guess_dict = attack_key_repeat(attack_traces_scaled_f,attack_traces_diffset_f,attack_traces_multi_f,mle_N)\n",
    "    g_guess_dict = attack_key_repeat(attack_traces_scaled_g,attack_traces_diffset_g,attack_traces_multi_g,mle_N)\n",
    " \n",
    "    re = check_key_right(f_guess_dict, g_guess_dict, filename_f, filename_g, key_th, try_random_num)\n",
    "    print(f\"the {key_th}th key guess's state is {re}\")\n",
    "    if re>0:\n",
    "        succ_num_count = succ_num_count + 1\n",
    "        key_key_idx_list.append(key_th)\n",
    "    else:\n",
    "        error_list.append(key_th)\n",
    "print(f\"the succ count is {succ_num_count}\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
