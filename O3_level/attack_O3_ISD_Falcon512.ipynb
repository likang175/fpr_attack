{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c2c4b1-3520-4717-b03c-e7cf3ed395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT = \"-O3\"\n",
    "Falcon_n = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aeab956-a81e-4f48-b367-5ef249ad13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sca_preprocess import trace_categorize\n",
    "from sca_preprocess import trace_fft, trace_butter_lpf\n",
    "from sca_preprocess import calc_snr, calc_ttest, calc_sod\n",
    "from sca_preprocess import select_poi_max_rank, select_poi_threshold\n",
    "from sca_preprocess import template_build, template_attack, template_attack_report\n",
    "from TA_discriminative import classifier_train, classifier_inference, classifier_report\n",
    "\n",
    "MCU_freq = 30e6\n",
    "adc_mul = 4\n",
    "sample_freq = MCU_freq * adc_mul\n",
    "cutoff_freq = sample_freq / 8\n",
    "\n",
    "seed = 42\n",
    "test_split = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752e70b0-4f8b-4208-a94b-c92044553979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_traces_accto_class(traces_dict, class_label_map):\n",
    "    final_traces = []\n",
    "    label = []\n",
    "    for cls in range(len(class_label_map.keys())):\n",
    "        N_trace = class_label_map[cls]['num_to_select']\n",
    "        label_range = class_label_map[cls]['labels']\n",
    "        traces_in_class = []\n",
    "        for l in label_range:\n",
    "            traces_in_class.append(traces_dict[l])\n",
    "            # print(l, traces_in_class[-1].shape)\n",
    "        combined = np.concatenate(traces_in_class, axis=0)  # ((1/2/4/8/16) x #trace, #sample)\n",
    "        # (1/2/4/8/16) x #trace ⪭ #trace，random pick #trace from it\n",
    "        selected_idx = np.random.choice(combined.shape[0], N_trace, replace=False)\n",
    "        selected = combined[selected_idx]\n",
    "        final_traces.append(selected)\n",
    "        label += [cls] * N_trace\n",
    "    traces = np.vstack(final_traces)  # shape (#class * #trace, #sample)\n",
    "    label = np.array(label)  # (#class * #trace,)\n",
    "    return traces, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99934aa8-2f5f-4293-a80c-2ece6e1731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_split(traces_dict, class_label_map, start_idx, end_idx, test_split,\n",
    "                     seed=seed, sample_freq=sample_freq, cutoff_freq=cutoff_freq):\n",
    "    # select traces in class\n",
    "    np.random.seed(seed)\n",
    "    traces, label = select_traces_accto_class(traces_dict, class_label_map)\n",
    "    traces = traces[:, start_idx:end_idx]\n",
    "    traces_lpf = trace_butter_lpf(traces, sample_freq, cutoff_freq)\n",
    "    # traces_dict = trace_categorize(traces, label)\n",
    "    print(f\"traces.shape: {traces.shape}\")\n",
    "    print(f\"label.shape: {label.shape}\")\n",
    "\n",
    "    # train/test split\n",
    "    traces_train, traces_test, label_train, label_test = \\\n",
    "        train_test_split(traces_lpf, label,\n",
    "                         test_size=test_split,\n",
    "                         stratify=label,\n",
    "                         shuffle=True,\n",
    "                         random_state=seed)\n",
    "    print(f\"traces_train.shape: {traces_train.shape}\")\n",
    "    print(f\"traces_test.shape: {traces_test.shape}\")\n",
    "\n",
    "    return traces_train, traces_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538a53e3-36a3-47fc-8b1a-54b53d0a9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_indices = {\n",
    "    'scaled': (0, 121),\n",
    "    'mul_shift': (41, 63),\n",
    "    'mul_multiply': (22, 39)\n",
    "}\n",
    "N_k_trans = 2\n",
    "N_trace = Falcon_n * N_k_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9262b638-ad40-4fab-9cb5-6bff77726c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect #trace/label = 100\n",
    "\n",
    "class_label_map_scaled = {\n",
    "    0: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [0]},\n",
    "    1: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [1]},\n",
    "    2: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-1]},\n",
    "    3: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [2, 3]},\n",
    "    4: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-3, -2]},\n",
    "    5: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [4, 5, 6, 7]},\n",
    "    6: {\"num_to_select\": N_trace,\n",
    "        \"labels\": [-7, -6, -5, -4]},\n",
    "    7: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(8, 16))},    # [8, ..., 15]\n",
    "    8: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(-15, -7))},  # [-15, ..., -8]\n",
    "    9: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(16, 23))},    # [16, ..., 22]\n",
    "    10: {\"num_to_select\": N_trace,\n",
    "        \"labels\": list(range(-22, -15))},  # [-22, ..., -16]\n",
    "}\n",
    "# N_class = len(class_label_map_scaled.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a82988-cf67-4425-a03e-cae4d052d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_map_shift = {\n",
    "    0: {\"num_to_select\": N_trace * 14,  # b.c. class 0 has 14 elements\n",
    "        \"labels\":  # make zu<2^55\n",
    "        # Based on the key distribution, it is virtually impossible to obtain a value larger than 22.\n",
    "        [2, 4, 5, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22]},\n",
    "    1: {\"num_to_select\": N_trace * 7,  # b.c. class 1 has 7 elements\n",
    "        \"labels\":  # make zu>=2^55\n",
    "        # [3, 6, 7, 12, 13, 14, 15, 23, 24, 25, 26, 27, 28, 29, 30, 31]},\n",
    "        [3, 6, 7, 12, 13, 14, 15]},\n",
    "}\n",
    "\n",
    "class_label_map_4_5 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [4]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [5]},\n",
    "}\n",
    "class_label_map_6_7 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [6]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [7]},\n",
    "}\n",
    "class_label_map_8_11 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [8]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [9]},\n",
    "    2: {\"num_to_select\": N_trace, \"labels\": [10]},\n",
    "    3: {\"num_to_select\": N_trace, \"labels\": [11]},\n",
    "}\n",
    "class_label_map_12_15 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [12]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [13]},\n",
    "    2: {\"num_to_select\": N_trace, \"labels\": [14]},\n",
    "    3: {\"num_to_select\": N_trace, \"labels\": [15]},\n",
    "}\n",
    "class_label_map_16_22 = {\n",
    "    0: {\"num_to_select\": N_trace, \"labels\": [16]},\n",
    "    1: {\"num_to_select\": N_trace, \"labels\": [17]},\n",
    "    2: {\"num_to_select\": N_trace, \"labels\": [18]},\n",
    "    3: {\"num_to_select\": N_trace, \"labels\": [19]},\n",
    "    4: {\"num_to_select\": N_trace, \"labels\": [20]},\n",
    "    5: {\"num_to_select\": N_trace, \"labels\": [21]},\n",
    "    6: {\"num_to_select\": N_trace, \"labels\": [22]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fb7abd-70f4-4ac2-9038-5a3f5dc95f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_from_csv(file_name, target_row, repeat_times, class_label_map):\n",
    "    value_to_key = {v: k for k, values in class_label_map.items() for v in values['labels']}\n",
    "    df = pd.read_csv(file_name, header=None, skiprows=target_row, nrows=1)\n",
    "    labels_2d = df.map(lambda x: value_to_key[x]).to_numpy()  # shape (1, 512)\n",
    "    labels_repeated_2d = np.tile(labels_2d, (1, repeat_times))  # shape (1, 512*repeat_times)\n",
    "    labels_scaled_key = labels_repeated_2d.flatten()\n",
    "    return labels_scaled_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875b59f6-17dd-4bc9-b264-66650f4a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel(nn.Module):\n",
    "    def __init__(self, N_class):\n",
    "        super().__init__()\n",
    "        k = 5\n",
    "        p = k // 2  # \"same\" padding\n",
    "        N_channel = 1\n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv1d(N_channel, 4, kernel_size=k, padding=p)\n",
    "        self.conv2 = nn.Conv1d(4, 4, kernel_size=k, padding=p)\n",
    "        self.bn1 = nn.BatchNorm1d(4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # MLP\n",
    "        self.fc1 = nn.LazyLinear(128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, N_class)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.act(self.bn2(self.conv2(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94288e1f-9ff1-44c2-854a-17cb9cf3054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fpr_templates_ml(traces_dict, clip_range,\n",
    "                           test_split, class_label_map, num_epochs=50):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    traces_train, traces_test, label_train, label_test = \\\n",
    "        select_and_split(traces_dict, class_label_map,\n",
    "                         0, clip_range[-1]+1, test_split)\n",
    "    N_class = len(class_label_map.keys())\n",
    "    model = NaiveModel(N_class)\n",
    "    model, scaler = classifier_train(traces_train, label_train,\\\n",
    "                                     clip_range, model)\n",
    "    y_prob = classifier_inference(traces_test,\n",
    "                             clip_range, model, scaler)\n",
    "    classifier_report(traces_test, label_test, clip_range, model, scaler,\n",
    "                     verbose=True)\n",
    "    # we actually don't need y_prob to attack; in case you need it to check bug, I also export it\n",
    "    return model, scaler, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ab53e6-2fba-4a77-9295-7debe606e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "for i in range(-22, 23):\n",
    "    label_all.extend([i] * N_trace)\n",
    "label_all = np.array(label_all)  # #trace x 45 (-22 to 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1db718c-935b-451c-9a81-0939fcd1075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_scaled = []\n",
    "len_scaled = 121 * 4\n",
    "for i in range(-22, 23):\n",
    "    # the collected traces corresponding to the fpr_scaled function\n",
    "    traces_scaled.append(np.load(f'data{OPT}-828/profile/scaled/'\n",
    "                                 f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_scaled))\n",
    "traces_scaled = np.array(traces_scaled).reshape(-1, len_scaled)\n",
    "traces_scaled_dict = trace_categorize(traces_scaled, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d3309e-eb58-4afa-9035-3e3a5fd61e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (11264, 484)\n",
      "label.shape: (11264,)\n",
      "traces_train.shape: (5632, 484)\n",
      "traces_test.shape: (5632, 484)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            1,419\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 18,055\n",
      "Trainable params: 18,055\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.9058 acc 0.5403 | val loss 1.0775 acc 0.9255\n",
      "Epoch 02 | train loss 0.3898 acc 0.9475 | val loss 0.0856 acc 0.9947\n",
      "Epoch 03 | train loss 0.0933 acc 0.9826 | val loss 0.0330 acc 0.9965\n",
      "Epoch 04 | train loss 0.0507 acc 0.9899 | val loss 0.0237 acc 1.0000\n",
      "Epoch 05 | train loss 0.0390 acc 0.9921 | val loss 0.0131 acc 1.0000\n",
      "Epoch 06 | train loss 0.0282 acc 0.9943 | val loss 0.0096 acc 1.0000\n",
      "Epoch 07 | train loss 0.0220 acc 0.9949 | val loss 0.0083 acc 0.9982\n",
      "Epoch 08 | train loss 0.0144 acc 0.9978 | val loss 0.0051 acc 1.0000\n",
      "Epoch 09 | train loss 0.0142 acc 0.9970 | val loss 0.0050 acc 1.0000\n",
      "Epoch 10 | train loss 0.0102 acc 0.9986 | val loss 0.0040 acc 1.0000\n",
      "Epoch 11 | train loss 0.0080 acc 0.9988 | val loss 0.0031 acc 1.0000\n",
      "Epoch 12 | train loss 0.0070 acc 0.9986 | val loss 0.0026 acc 1.0000\n",
      "Epoch 13 | train loss 0.0058 acc 0.9990 | val loss 0.0018 acc 1.0000\n",
      "Epoch 14 | train loss 0.0027 acc 1.0000 | val loss 0.0012 acc 1.0000\n",
      "Epoch 15 | train loss 0.0036 acc 0.9996 | val loss 0.0012 acc 1.0000\n",
      "Epoch 16 | train loss 0.0033 acc 0.9994 | val loss 0.0009 acc 1.0000\n",
      "Epoch 17 | train loss 0.0051 acc 0.9990 | val loss 0.0014 acc 1.0000\n",
      "Epoch 18 | train loss 0.0034 acc 0.9994 | val loss 0.0008 acc 1.0000\n",
      "Epoch 19 | train loss 0.0028 acc 0.9996 | val loss 0.0006 acc 1.0000\n",
      "Epoch 20 | train loss 0.0022 acc 0.9998 | val loss 0.0007 acc 1.0000\n",
      "Epoch 21 | train loss 0.0030 acc 0.9994 | val loss 0.0007 acc 1.0000\n",
      "Epoch 22 | train loss 0.0024 acc 0.9992 | val loss 0.0008 acc 1.0000\n",
      "Epoch 23 | train loss 0.0020 acc 0.9998 | val loss 0.0010 acc 1.0000\n",
      "Epoch 24 | train loss 0.0018 acc 0.9996 | val loss 0.0006 acc 1.0000\n",
      "Epoch 25 | train loss 0.0011 acc 1.0000 | val loss 0.0003 acc 1.0000\n",
      "Epoch 26 | train loss 0.0008 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 27 | train loss 0.0011 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 28 | train loss 0.0030 acc 0.9990 | val loss 0.0005 acc 1.0000\n",
      "Epoch 29 | train loss 0.0027 acc 0.9994 | val loss 0.0005 acc 1.0000\n",
      "Epoch 30 | train loss 0.0010 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 31 | train loss 0.0016 acc 0.9994 | val loss 0.0003 acc 1.0000\n",
      "Epoch 32 | train loss 0.0012 acc 0.9996 | val loss 0.0001 acc 1.0000\n",
      "Epoch 33 | train loss 0.0016 acc 0.9996 | val loss 0.0003 acc 1.0000\n",
      "Epoch 34 | train loss 0.0011 acc 1.0000 | val loss 0.0003 acc 1.0000\n",
      "Epoch 35 | train loss 0.0014 acc 0.9994 | val loss 0.0002 acc 1.0000\n",
      "Epoch 36 | train loss 0.0008 acc 0.9998 | val loss 0.0004 acc 1.0000\n",
      "Epoch 37 | train loss 0.0006 acc 1.0000 | val loss 0.0002 acc 1.0000\n",
      "Epoch 38 | train loss 0.0006 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 39 | train loss 0.0003 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 40 | train loss 0.0009 acc 0.9998 | val loss 0.0001 acc 1.0000\n",
      "Epoch 41 | train loss 0.0004 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 42 | train loss 0.0004 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 43 | train loss 0.0010 acc 0.9998 | val loss 0.0080 acc 0.9947\n",
      "Epoch 44 | train loss 0.0051 acc 0.9976 | val loss 0.0007 acc 1.0000\n",
      "Epoch 45 | train loss 0.0013 acc 0.9998 | val loss 0.0002 acc 1.0000\n",
      "Epoch 46 | train loss 0.0005 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 47 | train loss 0.0004 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 48 | train loss 0.0006 acc 1.0000 | val loss 0.0008 acc 1.0000\n",
      "Epoch 49 | train loss 0.0005 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Epoch 50 | train loss 0.0002 acc 1.0000 | val loss 0.0001 acc 1.0000\n",
      "Test set LOSS: 0.0003\n",
      "Test set ACCURACY: 1.0000\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   1.000000  1.000000  1.000000       512\n",
      "           1   1.000000  1.000000  1.000000       512\n",
      "           2   1.000000  1.000000  1.000000       512\n",
      "           3   1.000000  1.000000  1.000000       512\n",
      "           4   1.000000  1.000000  1.000000       512\n",
      "           5   1.000000  1.000000  1.000000       512\n",
      "           6   1.000000  1.000000  1.000000       512\n",
      "           7   1.000000  1.000000  1.000000       512\n",
      "           8   1.000000  1.000000  1.000000       512\n",
      "           9   1.000000  1.000000  1.000000       512\n",
      "          10   1.000000  1.000000  1.000000       512\n",
      "\n",
      "    accuracy                       1.000000      5632\n",
      "   macro avg   1.000000  1.000000  1.000000      5632\n",
      "weighted avg   1.000000  1.000000  1.000000      5632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_scaled, scaler_scaled, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_scaled_dict, range(len_scaled),\n",
    "                           test_split, class_label_map_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b6d95c-8627-4554-ac5f-d871b747c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_shift = []\n",
    "len_shift = 22 * 4\n",
    "for i in range(-22, 23):\n",
    "    # the collected traces corresponding to the normalization procedure in fpr_mul\n",
    "    traces_shift.append(np.load(f'data{OPT}-828/profile/shift/'\n",
    "                                f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_shift))\n",
    "traces_shift = np.array(traces_shift).reshape(-1, len_shift)\n",
    "traces_shift_dict = trace_categorize(traces_shift, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb76d2af-fdb2-4a58-9219-c2e3ddeb9ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (21504, 88)\n",
      "label.shape: (21504,)\n",
      "traces_train.shape: (10752, 88)\n",
      "traces_test.shape: (10752, 88)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.5831 acc 0.6854 | val loss 0.4853 acc 0.7639\n",
      "Epoch 02 | train loss 0.4501 acc 0.7875 | val loss 0.4016 acc 0.8225\n",
      "Epoch 03 | train loss 0.4030 acc 0.8184 | val loss 0.3702 acc 0.8457\n",
      "Epoch 04 | train loss 0.3653 acc 0.8382 | val loss 0.3520 acc 0.8559\n",
      "Epoch 05 | train loss 0.3556 acc 0.8413 | val loss 0.3442 acc 0.8550\n",
      "Epoch 06 | train loss 0.3471 acc 0.8468 | val loss 0.3373 acc 0.8587\n",
      "Epoch 07 | train loss 0.3413 acc 0.8489 | val loss 0.3276 acc 0.8671\n",
      "Epoch 08 | train loss 0.3306 acc 0.8551 | val loss 0.3248 acc 0.8699\n",
      "Epoch 09 | train loss 0.3185 acc 0.8606 | val loss 0.3074 acc 0.8699\n",
      "Epoch 10 | train loss 0.3179 acc 0.8621 | val loss 0.3052 acc 0.8773\n",
      "Epoch 11 | train loss 0.3162 acc 0.8639 | val loss 0.3022 acc 0.8708\n",
      "Epoch 12 | train loss 0.3085 acc 0.8685 | val loss 0.2962 acc 0.8792\n",
      "Epoch 13 | train loss 0.2983 acc 0.8723 | val loss 0.3036 acc 0.8690\n",
      "Epoch 14 | train loss 0.2957 acc 0.8747 | val loss 0.3011 acc 0.8699\n",
      "Epoch 15 | train loss 0.2912 acc 0.8757 | val loss 0.2831 acc 0.8857\n",
      "Epoch 16 | train loss 0.2955 acc 0.8722 | val loss 0.2867 acc 0.8764\n",
      "Epoch 17 | train loss 0.2760 acc 0.8832 | val loss 0.2844 acc 0.8773\n",
      "Epoch 18 | train loss 0.2793 acc 0.8800 | val loss 0.2750 acc 0.8903\n",
      "Epoch 19 | train loss 0.2700 acc 0.8833 | val loss 0.2673 acc 0.8903\n",
      "Epoch 20 | train loss 0.2716 acc 0.8834 | val loss 0.2679 acc 0.8857\n",
      "Epoch 21 | train loss 0.2648 acc 0.8850 | val loss 0.2810 acc 0.8783\n",
      "Epoch 22 | train loss 0.2675 acc 0.8875 | val loss 0.2704 acc 0.8968\n",
      "Epoch 23 | train loss 0.2567 acc 0.8894 | val loss 0.2720 acc 0.8866\n",
      "Epoch 24 | train loss 0.2609 acc 0.8868 | val loss 0.2487 acc 0.8978\n",
      "Epoch 25 | train loss 0.2518 acc 0.8932 | val loss 0.2616 acc 0.8885\n",
      "Epoch 26 | train loss 0.2520 acc 0.8929 | val loss 0.2447 acc 0.9043\n",
      "Epoch 27 | train loss 0.2417 acc 0.8949 | val loss 0.2453 acc 0.9052\n",
      "Epoch 28 | train loss 0.2439 acc 0.8975 | val loss 0.2370 acc 0.9089\n",
      "Epoch 29 | train loss 0.2349 acc 0.8999 | val loss 0.2416 acc 0.9006\n",
      "Epoch 30 | train loss 0.2352 acc 0.9021 | val loss 0.2496 acc 0.8885\n",
      "Epoch 31 | train loss 0.2366 acc 0.9003 | val loss 0.2547 acc 0.8959\n",
      "Epoch 32 | train loss 0.2342 acc 0.9017 | val loss 0.2381 acc 0.9061\n",
      "Epoch 33 | train loss 0.2304 acc 0.9032 | val loss 0.2303 acc 0.9099\n",
      "Epoch 34 | train loss 0.2190 acc 0.9107 | val loss 0.2364 acc 0.9006\n",
      "Epoch 35 | train loss 0.2218 acc 0.9072 | val loss 0.2435 acc 0.9052\n",
      "Epoch 36 | train loss 0.2172 acc 0.9100 | val loss 0.2258 acc 0.9099\n",
      "Epoch 37 | train loss 0.2192 acc 0.9081 | val loss 0.2399 acc 0.8996\n",
      "Epoch 38 | train loss 0.2155 acc 0.9086 | val loss 0.2258 acc 0.9136\n",
      "Epoch 39 | train loss 0.2115 acc 0.9131 | val loss 0.2365 acc 0.9043\n",
      "Epoch 40 | train loss 0.2050 acc 0.9131 | val loss 0.2332 acc 0.9117\n",
      "Epoch 41 | train loss 0.2096 acc 0.9122 | val loss 0.2427 acc 0.9043\n",
      "Epoch 42 | train loss 0.2082 acc 0.9137 | val loss 0.2280 acc 0.9043\n",
      "Epoch 43 | train loss 0.2036 acc 0.9166 | val loss 0.2258 acc 0.9145\n",
      "Epoch 44 | train loss 0.2040 acc 0.9150 | val loss 0.2239 acc 0.9182\n",
      "Epoch 45 | train loss 0.1983 acc 0.9193 | val loss 0.2322 acc 0.9136\n",
      "Epoch 46 | train loss 0.1965 acc 0.9181 | val loss 0.2550 acc 0.9071\n",
      "Epoch 47 | train loss 0.1970 acc 0.9175 | val loss 0.2272 acc 0.9117\n",
      "Epoch 48 | train loss 0.1947 acc 0.9202 | val loss 0.2233 acc 0.9080\n",
      "Epoch 49 | train loss 0.1979 acc 0.9175 | val loss 0.2235 acc 0.9071\n",
      "Epoch 50 | train loss 0.1916 acc 0.9195 | val loss 0.2568 acc 0.8968\n",
      "Test set LOSS: 0.2421\n",
      "Test set ACCURACY: 0.8993\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.929307  0.918806  0.924027      7168\n",
      "           1   0.841201  0.860212  0.850600      3584\n",
      "\n",
      "    accuracy                       0.899275     10752\n",
      "   macro avg   0.885254  0.889509  0.887313     10752\n",
      "weighted avg   0.899938  0.899275  0.899551     10752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_shift, scaler_shift, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_shift_dict, range(len_shift),\n",
    "                           test_split, class_label_map_shift, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f1aaf4-b20c-4ce5-b375-abb8e25c810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_mul = []\n",
    "len_mul = 17 * 4\n",
    "for i in range(-22, 23):\n",
    "    # the collected traces corresponding to the mantissa multiplication in fpr_mul\n",
    "    traces_mul.append(np.load(f'data{OPT}-828/profile/mul/'\n",
    "                              f'val{i}_{N_k_trans}traces.npy').reshape(-1, len_mul))\n",
    "traces_mul = np.array(traces_mul).reshape(-1, len_mul)\n",
    "traces_mul_dict = trace_categorize(traces_mul, label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "436bbc82-7313-47e1-943e-1f5304b683eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces.shape: (2048, 68)\n",
      "label.shape: (2048,)\n",
      "traces_train.shape: (1024, 68)\n",
      "traces_test.shape: (1024, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.6562 acc 0.7611 | val loss 0.6770 acc 0.8058\n",
      "Epoch 02 | train loss 0.5186 acc 0.9805 | val loss 0.5908 acc 0.9709\n",
      "Epoch 03 | train loss 0.2900 acc 0.9848 | val loss 0.3438 acc 0.9806\n",
      "Epoch 04 | train loss 0.1090 acc 0.9891 | val loss 0.1104 acc 0.9903\n",
      "Epoch 05 | train loss 0.0418 acc 0.9924 | val loss 0.0571 acc 0.9903\n",
      "Epoch 06 | train loss 0.0272 acc 0.9924 | val loss 0.0555 acc 0.9903\n",
      "Epoch 07 | train loss 0.0183 acc 0.9946 | val loss 0.0617 acc 0.9903\n",
      "Epoch 08 | train loss 0.0170 acc 0.9946 | val loss 0.0628 acc 0.9903\n",
      "Epoch 09 | train loss 0.0121 acc 0.9967 | val loss 0.0608 acc 0.9903\n",
      "Epoch 10 | train loss 0.0131 acc 0.9957 | val loss 0.0721 acc 0.9903\n",
      "Epoch 11 | train loss 0.0150 acc 0.9967 | val loss 0.0729 acc 0.9903\n",
      "Epoch 12 | train loss 0.0121 acc 0.9957 | val loss 0.0653 acc 0.9903\n",
      "Epoch 13 | train loss 0.0086 acc 0.9978 | val loss 0.0773 acc 0.9903\n",
      "Epoch 14 | train loss 0.0099 acc 0.9967 | val loss 0.0626 acc 0.9903\n",
      "Epoch 15 | train loss 0.0134 acc 0.9957 | val loss 0.0821 acc 0.9903\n",
      "Epoch 16 | train loss 0.0087 acc 0.9978 | val loss 0.0658 acc 0.9903\n",
      "Epoch 17 | train loss 0.0096 acc 0.9967 | val loss 0.0716 acc 0.9903\n",
      "Epoch 18 | train loss 0.0039 acc 1.0000 | val loss 0.0785 acc 0.9903\n",
      "Epoch 19 | train loss 0.0095 acc 0.9957 | val loss 0.0758 acc 0.9903\n",
      "Epoch 20 | train loss 0.0070 acc 0.9978 | val loss 0.0816 acc 0.9903\n",
      "Epoch 21 | train loss 0.0075 acc 0.9978 | val loss 0.0750 acc 0.9903\n",
      "Epoch 22 | train loss 0.0076 acc 0.9967 | val loss 0.0836 acc 0.9903\n",
      "Early stopping triggered.\n",
      "Test set LOSS: 0.0386\n",
      "Test set ACCURACY: 0.9844\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.982490  0.986328  0.984405       512\n",
      "           1   0.986275  0.982422  0.984344       512\n",
      "\n",
      "    accuracy                       0.984375      1024\n",
      "   macro avg   0.984382  0.984375  0.984375      1024\n",
      "weighted avg   0.984382  0.984375  0.984375      1024\n",
      "\n",
      "traces.shape: (2048, 68)\n",
      "label.shape: (2048,)\n",
      "traces_train.shape: (1024, 68)\n",
      "traces_test.shape: (1024, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            258\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 16,894\n",
      "Trainable params: 16,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 0.6772 acc 0.6721 | val loss 0.6870 acc 0.7670\n",
      "Epoch 02 | train loss 0.6164 acc 0.8371 | val loss 0.6584 acc 0.6311\n",
      "Epoch 03 | train loss 0.5067 acc 0.8502 | val loss 0.5733 acc 0.7379\n",
      "Epoch 04 | train loss 0.3869 acc 0.8578 | val loss 0.4337 acc 0.8058\n",
      "Epoch 05 | train loss 0.2996 acc 0.8730 | val loss 0.3719 acc 0.8155\n",
      "Epoch 06 | train loss 0.2744 acc 0.8882 | val loss 0.3518 acc 0.8252\n",
      "Epoch 07 | train loss 0.2600 acc 0.8936 | val loss 0.4078 acc 0.8350\n",
      "Epoch 08 | train loss 0.2639 acc 0.9012 | val loss 0.3891 acc 0.8350\n",
      "Epoch 09 | train loss 0.2404 acc 0.9045 | val loss 0.3259 acc 0.8544\n",
      "Epoch 10 | train loss 0.2511 acc 0.8936 | val loss 0.4369 acc 0.8155\n",
      "Epoch 11 | train loss 0.2581 acc 0.8979 | val loss 0.3352 acc 0.8544\n",
      "Epoch 12 | train loss 0.2301 acc 0.9153 | val loss 0.3334 acc 0.8447\n",
      "Epoch 13 | train loss 0.2273 acc 0.9121 | val loss 0.3492 acc 0.8350\n",
      "Epoch 14 | train loss 0.2262 acc 0.9153 | val loss 0.3104 acc 0.8641\n",
      "Epoch 15 | train loss 0.2267 acc 0.9164 | val loss 0.3779 acc 0.8447\n",
      "Epoch 16 | train loss 0.2234 acc 0.9066 | val loss 0.3555 acc 0.8350\n",
      "Epoch 17 | train loss 0.2110 acc 0.9164 | val loss 0.3272 acc 0.8544\n",
      "Epoch 18 | train loss 0.2153 acc 0.9186 | val loss 0.3522 acc 0.8447\n",
      "Epoch 19 | train loss 0.2183 acc 0.9175 | val loss 0.3596 acc 0.8447\n",
      "Epoch 20 | train loss 0.2172 acc 0.9131 | val loss 0.3010 acc 0.8641\n",
      "Epoch 21 | train loss 0.2104 acc 0.9164 | val loss 0.3546 acc 0.8447\n",
      "Epoch 22 | train loss 0.2118 acc 0.9175 | val loss 0.2922 acc 0.8641\n",
      "Epoch 23 | train loss 0.2150 acc 0.9186 | val loss 0.3659 acc 0.8447\n",
      "Epoch 24 | train loss 0.2088 acc 0.9153 | val loss 0.3056 acc 0.8447\n",
      "Epoch 25 | train loss 0.2010 acc 0.9229 | val loss 0.3104 acc 0.8447\n",
      "Epoch 26 | train loss 0.1942 acc 0.9207 | val loss 0.3260 acc 0.8447\n",
      "Epoch 27 | train loss 0.2069 acc 0.9262 | val loss 0.2925 acc 0.8641\n",
      "Epoch 28 | train loss 0.2026 acc 0.9240 | val loss 0.3251 acc 0.8447\n",
      "Epoch 29 | train loss 0.2130 acc 0.9175 | val loss 0.3634 acc 0.8544\n",
      "Epoch 30 | train loss 0.1969 acc 0.9229 | val loss 0.2977 acc 0.8835\n",
      "Epoch 31 | train loss 0.2051 acc 0.9153 | val loss 0.3104 acc 0.8544\n",
      "Epoch 32 | train loss 0.1921 acc 0.9218 | val loss 0.3302 acc 0.8447\n",
      "Epoch 33 | train loss 0.1973 acc 0.9240 | val loss 0.2909 acc 0.8835\n",
      "Epoch 34 | train loss 0.1924 acc 0.9251 | val loss 0.3572 acc 0.8447\n",
      "Epoch 35 | train loss 0.1937 acc 0.9207 | val loss 0.3159 acc 0.8544\n",
      "Epoch 36 | train loss 0.1864 acc 0.9218 | val loss 0.2988 acc 0.8835\n",
      "Epoch 37 | train loss 0.1822 acc 0.9262 | val loss 0.3284 acc 0.8544\n",
      "Epoch 38 | train loss 0.1799 acc 0.9251 | val loss 0.3018 acc 0.8835\n",
      "Epoch 39 | train loss 0.1846 acc 0.9316 | val loss 0.3158 acc 0.8738\n",
      "Epoch 40 | train loss 0.1788 acc 0.9273 | val loss 0.3377 acc 0.8738\n",
      "Epoch 41 | train loss 0.1792 acc 0.9327 | val loss 0.3084 acc 0.8738\n",
      "Epoch 42 | train loss 0.1776 acc 0.9327 | val loss 0.2930 acc 0.8835\n",
      "Epoch 43 | train loss 0.1712 acc 0.9283 | val loss 0.3002 acc 0.8835\n",
      "Epoch 44 | train loss 0.1746 acc 0.9316 | val loss 0.3073 acc 0.8835\n",
      "Epoch 45 | train loss 0.1647 acc 0.9316 | val loss 0.3010 acc 0.8835\n",
      "Epoch 46 | train loss 0.1709 acc 0.9294 | val loss 0.3178 acc 0.8932\n",
      "Epoch 47 | train loss 0.1785 acc 0.9305 | val loss 0.2793 acc 0.8835\n",
      "Epoch 48 | train loss 0.1830 acc 0.9283 | val loss 0.2940 acc 0.8932\n",
      "Epoch 49 | train loss 0.1618 acc 0.9403 | val loss 0.3069 acc 0.8932\n",
      "Epoch 50 | train loss 0.1666 acc 0.9327 | val loss 0.3083 acc 0.8835\n",
      "Test set LOSS: 0.2400\n",
      "Test set ACCURACY: 0.9092\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.933747  0.880859  0.906533       512\n",
      "           1   0.887246  0.937500  0.911681       512\n",
      "\n",
      "    accuracy                       0.909180      1024\n",
      "   macro avg   0.910497  0.909180  0.909107      1024\n",
      "weighted avg   0.910497  0.909180  0.909107      1024\n",
      "\n",
      "traces.shape: (4096, 68)\n",
      "label.shape: (4096,)\n",
      "traces_train.shape: (2048, 68)\n",
      "traces_test.shape: (2048, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            516\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,152\n",
      "Trainable params: 17,152\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.3555 acc 0.3890 | val loss 1.3666 acc 0.2537\n",
      "Epoch 02 | train loss 1.2021 acc 0.5703 | val loss 1.1612 acc 0.6341\n",
      "Epoch 03 | train loss 0.9045 acc 0.6343 | val loss 0.7890 acc 0.6976\n",
      "Epoch 04 | train loss 0.7003 acc 0.7027 | val loss 0.6229 acc 0.7268\n",
      "Epoch 05 | train loss 0.6192 acc 0.7238 | val loss 0.5453 acc 0.7805\n",
      "Epoch 06 | train loss 0.5786 acc 0.7499 | val loss 0.5114 acc 0.7756\n",
      "Epoch 07 | train loss 0.5522 acc 0.7623 | val loss 0.4902 acc 0.8000\n",
      "Epoch 08 | train loss 0.5250 acc 0.7748 | val loss 0.4772 acc 0.8000\n",
      "Epoch 09 | train loss 0.5080 acc 0.7868 | val loss 0.4549 acc 0.8098\n",
      "Epoch 10 | train loss 0.4988 acc 0.7797 | val loss 0.4453 acc 0.7951\n",
      "Epoch 11 | train loss 0.4856 acc 0.7933 | val loss 0.4440 acc 0.8000\n",
      "Epoch 12 | train loss 0.4845 acc 0.7835 | val loss 0.4512 acc 0.8000\n",
      "Epoch 13 | train loss 0.4795 acc 0.7944 | val loss 0.4582 acc 0.8000\n",
      "Epoch 14 | train loss 0.4834 acc 0.7922 | val loss 0.4266 acc 0.8244\n",
      "Epoch 15 | train loss 0.4577 acc 0.8030 | val loss 0.4274 acc 0.8195\n",
      "Epoch 16 | train loss 0.4608 acc 0.8090 | val loss 0.4270 acc 0.8195\n",
      "Epoch 17 | train loss 0.4485 acc 0.8106 | val loss 0.4316 acc 0.8146\n",
      "Epoch 18 | train loss 0.4442 acc 0.8128 | val loss 0.4232 acc 0.8195\n",
      "Epoch 19 | train loss 0.4484 acc 0.8079 | val loss 0.4250 acc 0.8146\n",
      "Epoch 20 | train loss 0.4319 acc 0.8166 | val loss 0.4199 acc 0.8195\n",
      "Epoch 21 | train loss 0.4340 acc 0.8155 | val loss 0.4156 acc 0.8244\n",
      "Epoch 22 | train loss 0.4206 acc 0.8182 | val loss 0.4123 acc 0.8146\n",
      "Epoch 23 | train loss 0.4206 acc 0.8226 | val loss 0.4185 acc 0.8146\n",
      "Epoch 24 | train loss 0.4214 acc 0.8117 | val loss 0.4115 acc 0.8098\n",
      "Epoch 25 | train loss 0.4055 acc 0.8253 | val loss 0.4076 acc 0.8195\n",
      "Epoch 26 | train loss 0.4106 acc 0.8285 | val loss 0.4137 acc 0.8000\n",
      "Epoch 27 | train loss 0.4192 acc 0.8133 | val loss 0.4964 acc 0.7756\n",
      "Epoch 28 | train loss 0.4236 acc 0.8204 | val loss 0.4130 acc 0.8098\n",
      "Epoch 29 | train loss 0.4233 acc 0.8237 | val loss 0.4119 acc 0.7854\n",
      "Epoch 30 | train loss 0.4183 acc 0.8123 | val loss 0.4095 acc 0.8049\n",
      "Epoch 31 | train loss 0.4063 acc 0.8177 | val loss 0.4081 acc 0.8098\n",
      "Epoch 32 | train loss 0.4063 acc 0.8166 | val loss 0.4098 acc 0.8098\n",
      "Epoch 33 | train loss 0.3939 acc 0.8291 | val loss 0.4163 acc 0.8000\n",
      "Epoch 34 | train loss 0.3945 acc 0.8291 | val loss 0.4399 acc 0.8244\n",
      "Epoch 35 | train loss 0.4073 acc 0.8139 | val loss 0.4099 acc 0.7805\n",
      "Epoch 36 | train loss 0.3865 acc 0.8345 | val loss 0.4147 acc 0.8195\n",
      "Epoch 37 | train loss 0.3803 acc 0.8318 | val loss 0.4054 acc 0.8195\n",
      "Epoch 38 | train loss 0.3622 acc 0.8470 | val loss 0.4146 acc 0.8098\n",
      "Epoch 39 | train loss 0.3814 acc 0.8383 | val loss 0.4047 acc 0.8146\n",
      "Epoch 40 | train loss 0.3838 acc 0.8448 | val loss 0.4052 acc 0.7951\n",
      "Epoch 41 | train loss 0.3782 acc 0.8351 | val loss 0.4049 acc 0.8098\n",
      "Epoch 42 | train loss 0.3661 acc 0.8410 | val loss 0.4056 acc 0.8293\n",
      "Epoch 43 | train loss 0.3763 acc 0.8361 | val loss 0.3981 acc 0.8098\n",
      "Epoch 44 | train loss 0.3768 acc 0.8432 | val loss 0.4070 acc 0.7951\n",
      "Epoch 45 | train loss 0.3734 acc 0.8421 | val loss 0.4412 acc 0.8049\n",
      "Epoch 46 | train loss 0.3763 acc 0.8388 | val loss 0.4121 acc 0.8146\n",
      "Epoch 47 | train loss 0.3768 acc 0.8345 | val loss 0.4055 acc 0.8244\n",
      "Epoch 48 | train loss 0.3627 acc 0.8432 | val loss 0.3982 acc 0.8049\n",
      "Epoch 49 | train loss 0.3622 acc 0.8530 | val loss 0.4102 acc 0.8146\n",
      "Epoch 50 | train loss 0.3650 acc 0.8388 | val loss 0.4078 acc 0.8244\n",
      "Test set LOSS: 0.3962\n",
      "Test set ACCURACY: 0.8354\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.782847  0.837891  0.809434       512\n",
      "           1   0.896624  0.830078  0.862069       512\n",
      "           2   0.941176  0.968750  0.954764       512\n",
      "           3   0.723447  0.705078  0.714144       512\n",
      "\n",
      "    accuracy                       0.835449      2048\n",
      "   macro avg   0.836024  0.835449  0.835103      2048\n",
      "weighted avg   0.836024  0.835449  0.835103      2048\n",
      "\n",
      "traces.shape: (4096, 68)\n",
      "label.shape: (4096,)\n",
      "traces_train.shape: (2048, 68)\n",
      "traces_test.shape: (2048, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            516\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,152\n",
      "Trainable params: 17,152\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.3570 acc 0.3668 | val loss 1.3598 acc 0.2488\n",
      "Epoch 02 | train loss 1.1726 acc 0.6012 | val loss 1.0977 acc 0.7902\n",
      "Epoch 03 | train loss 0.8312 acc 0.7396 | val loss 0.6855 acc 0.7610\n",
      "Epoch 04 | train loss 0.6122 acc 0.7840 | val loss 0.5085 acc 0.7951\n",
      "Epoch 05 | train loss 0.5091 acc 0.8085 | val loss 0.4617 acc 0.8000\n",
      "Epoch 06 | train loss 0.4545 acc 0.8334 | val loss 0.4296 acc 0.8195\n",
      "Epoch 07 | train loss 0.4316 acc 0.8367 | val loss 0.3887 acc 0.8390\n",
      "Epoch 08 | train loss 0.4022 acc 0.8497 | val loss 0.3581 acc 0.8537\n",
      "Epoch 09 | train loss 0.3882 acc 0.8497 | val loss 0.3933 acc 0.8098\n",
      "Epoch 10 | train loss 0.3744 acc 0.8589 | val loss 0.3618 acc 0.8439\n",
      "Epoch 11 | train loss 0.3614 acc 0.8600 | val loss 0.3436 acc 0.8488\n",
      "Epoch 12 | train loss 0.3574 acc 0.8568 | val loss 0.3165 acc 0.8732\n",
      "Epoch 13 | train loss 0.3572 acc 0.8654 | val loss 0.3228 acc 0.8976\n",
      "Epoch 14 | train loss 0.3504 acc 0.8676 | val loss 0.3262 acc 0.8829\n",
      "Epoch 15 | train loss 0.3498 acc 0.8638 | val loss 0.3248 acc 0.8732\n",
      "Epoch 16 | train loss 0.3176 acc 0.8714 | val loss 0.2992 acc 0.8829\n",
      "Epoch 17 | train loss 0.3210 acc 0.8774 | val loss 0.3024 acc 0.8634\n",
      "Epoch 18 | train loss 0.3222 acc 0.8736 | val loss 0.3075 acc 0.8732\n",
      "Epoch 19 | train loss 0.3240 acc 0.8741 | val loss 0.3106 acc 0.8537\n",
      "Epoch 20 | train loss 0.3158 acc 0.8736 | val loss 0.2884 acc 0.8780\n",
      "Epoch 21 | train loss 0.3077 acc 0.8812 | val loss 0.2835 acc 0.8829\n",
      "Epoch 22 | train loss 0.3081 acc 0.8790 | val loss 0.2857 acc 0.8537\n",
      "Epoch 23 | train loss 0.3027 acc 0.8871 | val loss 0.2657 acc 0.8927\n",
      "Epoch 24 | train loss 0.2903 acc 0.8812 | val loss 0.2889 acc 0.8780\n",
      "Epoch 25 | train loss 0.2934 acc 0.8828 | val loss 0.2909 acc 0.8634\n",
      "Epoch 26 | train loss 0.2915 acc 0.8866 | val loss 0.2844 acc 0.8634\n",
      "Epoch 27 | train loss 0.2904 acc 0.8817 | val loss 0.2608 acc 0.8976\n",
      "Epoch 28 | train loss 0.2875 acc 0.8882 | val loss 0.2729 acc 0.8634\n",
      "Epoch 29 | train loss 0.2887 acc 0.8904 | val loss 0.2819 acc 0.8537\n",
      "Epoch 30 | train loss 0.2819 acc 0.8942 | val loss 0.2598 acc 0.8780\n",
      "Epoch 31 | train loss 0.2862 acc 0.8893 | val loss 0.2655 acc 0.8732\n",
      "Epoch 32 | train loss 0.2740 acc 0.8904 | val loss 0.2767 acc 0.8585\n",
      "Epoch 33 | train loss 0.2829 acc 0.8942 | val loss 0.2566 acc 0.8829\n",
      "Epoch 34 | train loss 0.2733 acc 0.8861 | val loss 0.2478 acc 0.8927\n",
      "Epoch 35 | train loss 0.2727 acc 0.8953 | val loss 0.2595 acc 0.8780\n",
      "Epoch 36 | train loss 0.2731 acc 0.8866 | val loss 0.2466 acc 0.8878\n",
      "Epoch 37 | train loss 0.2640 acc 0.8909 | val loss 0.2557 acc 0.8585\n",
      "Epoch 38 | train loss 0.2606 acc 0.8920 | val loss 0.2384 acc 0.9122\n",
      "Epoch 39 | train loss 0.2632 acc 0.8958 | val loss 0.2442 acc 0.8878\n",
      "Epoch 40 | train loss 0.2583 acc 0.9012 | val loss 0.2732 acc 0.8780\n",
      "Epoch 41 | train loss 0.2648 acc 0.9012 | val loss 0.2634 acc 0.8537\n",
      "Epoch 42 | train loss 0.2662 acc 0.8909 | val loss 0.2359 acc 0.8976\n",
      "Epoch 43 | train loss 0.2671 acc 0.8953 | val loss 0.2432 acc 0.9024\n",
      "Epoch 44 | train loss 0.2493 acc 0.9034 | val loss 0.2703 acc 0.8683\n",
      "Epoch 45 | train loss 0.2513 acc 0.9029 | val loss 0.2465 acc 0.9073\n",
      "Epoch 46 | train loss 0.2491 acc 0.9012 | val loss 0.2237 acc 0.9073\n",
      "Epoch 47 | train loss 0.2477 acc 0.9045 | val loss 0.2191 acc 0.9122\n",
      "Epoch 48 | train loss 0.2452 acc 0.9050 | val loss 0.2240 acc 0.9024\n",
      "Epoch 49 | train loss 0.2469 acc 0.8996 | val loss 0.2332 acc 0.8927\n",
      "Epoch 50 | train loss 0.2495 acc 0.9018 | val loss 0.2248 acc 0.8976\n",
      "Test set LOSS: 0.3050\n",
      "Test set ACCURACY: 0.8896\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.896761  0.865234  0.880716       512\n",
      "           1   0.925620  0.875000  0.899598       512\n",
      "           2   0.823308  0.855469  0.839080       512\n",
      "           3   0.916357  0.962891  0.939048       512\n",
      "\n",
      "    accuracy                       0.889648      2048\n",
      "   macro avg   0.890512  0.889648  0.889611      2048\n",
      "weighted avg   0.890512  0.889648  0.889611      2048\n",
      "\n",
      "traces.shape: (7168, 68)\n",
      "label.shape: (7168,)\n",
      "traces_train.shape: (3584, 68)\n",
      "traces_test.shape: (3584, 68)\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "NaiveModel                               --\n",
      "├─Conv1d: 1-1                            24\n",
      "├─Conv1d: 1-2                            84\n",
      "├─BatchNorm1d: 1-3                       8\n",
      "├─BatchNorm1d: 1-4                       8\n",
      "├─Sigmoid: 1-5                           --\n",
      "├─AvgPool1d: 1-6                         --\n",
      "├─LazyLinear: 1-7                        --\n",
      "├─Linear: 1-8                            16,512\n",
      "├─Linear: 1-9                            903\n",
      "├─Dropout: 1-10                          --\n",
      "├─ReLU: 1-11                             --\n",
      "=================================================================\n",
      "Total params: 17,539\n",
      "Trainable params: 17,539\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Epoch 01 | train loss 1.9112 acc 0.2453 | val loss 1.8842 acc 0.3454\n",
      "Epoch 02 | train loss 1.6373 acc 0.3653 | val loss 1.4439 acc 0.4234\n",
      "Epoch 03 | train loss 1.3482 acc 0.4453 | val loss 1.2896 acc 0.4401\n",
      "Epoch 04 | train loss 1.2160 acc 0.5079 | val loss 1.1924 acc 0.4903\n",
      "Epoch 05 | train loss 1.1575 acc 0.5333 | val loss 1.1652 acc 0.5404\n",
      "Epoch 06 | train loss 1.1293 acc 0.5488 | val loss 1.1187 acc 0.5014\n",
      "Epoch 07 | train loss 1.0956 acc 0.5572 | val loss 1.1222 acc 0.5292\n",
      "Epoch 08 | train loss 1.0752 acc 0.5569 | val loss 1.1196 acc 0.5265\n",
      "Epoch 09 | train loss 1.0449 acc 0.5867 | val loss 1.0752 acc 0.5599\n",
      "Epoch 10 | train loss 1.0276 acc 0.5913 | val loss 1.0757 acc 0.5738\n",
      "Epoch 11 | train loss 1.0230 acc 0.5882 | val loss 1.0359 acc 0.5738\n",
      "Epoch 12 | train loss 1.0012 acc 0.5941 | val loss 1.0195 acc 0.5877\n",
      "Epoch 13 | train loss 0.9836 acc 0.6016 | val loss 1.0771 acc 0.5320\n",
      "Epoch 14 | train loss 0.9741 acc 0.6040 | val loss 1.0016 acc 0.6184\n",
      "Epoch 15 | train loss 0.9596 acc 0.6093 | val loss 1.0390 acc 0.5905\n",
      "Epoch 16 | train loss 0.9660 acc 0.6093 | val loss 1.0020 acc 0.5961\n",
      "Epoch 17 | train loss 0.9414 acc 0.6239 | val loss 0.9804 acc 0.6267\n",
      "Epoch 18 | train loss 0.9428 acc 0.6242 | val loss 0.9625 acc 0.6128\n",
      "Epoch 19 | train loss 0.9338 acc 0.6208 | val loss 0.9676 acc 0.6212\n",
      "Epoch 20 | train loss 0.9227 acc 0.6208 | val loss 0.9689 acc 0.6072\n",
      "Epoch 21 | train loss 0.9283 acc 0.6211 | val loss 0.9458 acc 0.6295\n",
      "Epoch 22 | train loss 0.9188 acc 0.6270 | val loss 0.9459 acc 0.6351\n",
      "Epoch 23 | train loss 0.9257 acc 0.6248 | val loss 0.9743 acc 0.5961\n",
      "Epoch 24 | train loss 0.9138 acc 0.6254 | val loss 0.9439 acc 0.6323\n",
      "Epoch 25 | train loss 0.8942 acc 0.6322 | val loss 0.9348 acc 0.6240\n",
      "Epoch 26 | train loss 0.8912 acc 0.6381 | val loss 0.9540 acc 0.6323\n",
      "Epoch 27 | train loss 0.8960 acc 0.6443 | val loss 0.9636 acc 0.6156\n",
      "Epoch 28 | train loss 0.8871 acc 0.6403 | val loss 0.9219 acc 0.6379\n",
      "Epoch 29 | train loss 0.8866 acc 0.6412 | val loss 0.9741 acc 0.5989\n",
      "Epoch 30 | train loss 0.8826 acc 0.6369 | val loss 0.9646 acc 0.6128\n",
      "Epoch 31 | train loss 0.8826 acc 0.6329 | val loss 0.9488 acc 0.6323\n",
      "Epoch 32 | train loss 0.8641 acc 0.6478 | val loss 0.9425 acc 0.6462\n",
      "Epoch 33 | train loss 0.8776 acc 0.6394 | val loss 0.9146 acc 0.6435\n",
      "Epoch 34 | train loss 0.8640 acc 0.6490 | val loss 0.9078 acc 0.6546\n",
      "Epoch 35 | train loss 0.8592 acc 0.6468 | val loss 0.9099 acc 0.6518\n",
      "Epoch 36 | train loss 0.8525 acc 0.6524 | val loss 0.8993 acc 0.6546\n",
      "Epoch 37 | train loss 0.8515 acc 0.6443 | val loss 0.9032 acc 0.6574\n",
      "Epoch 38 | train loss 0.8514 acc 0.6546 | val loss 0.9095 acc 0.6379\n",
      "Epoch 39 | train loss 0.8525 acc 0.6499 | val loss 0.8920 acc 0.6630\n",
      "Epoch 40 | train loss 0.8446 acc 0.6598 | val loss 0.8958 acc 0.6574\n",
      "Epoch 41 | train loss 0.8346 acc 0.6592 | val loss 0.9108 acc 0.6518\n",
      "Epoch 42 | train loss 0.8337 acc 0.6667 | val loss 0.8830 acc 0.6741\n",
      "Epoch 43 | train loss 0.8313 acc 0.6583 | val loss 0.8864 acc 0.6685\n",
      "Epoch 44 | train loss 0.8137 acc 0.6707 | val loss 0.8815 acc 0.6685\n",
      "Epoch 45 | train loss 0.8304 acc 0.6620 | val loss 0.8989 acc 0.6630\n",
      "Epoch 46 | train loss 0.8249 acc 0.6682 | val loss 0.8887 acc 0.6685\n",
      "Epoch 47 | train loss 0.8136 acc 0.6685 | val loss 0.8768 acc 0.6685\n",
      "Epoch 48 | train loss 0.8275 acc 0.6645 | val loss 0.8856 acc 0.6630\n",
      "Epoch 49 | train loss 0.8228 acc 0.6642 | val loss 0.9428 acc 0.6462\n",
      "Epoch 50 | train loss 0.8157 acc 0.6750 | val loss 0.8805 acc 0.6602\n",
      "Test set LOSS: 0.8628\n",
      "Test set ACCURACY: 0.6498\n",
      "Test set classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.806202  0.812500  0.809339       512\n",
      "           1   0.665127  0.562500  0.609524       512\n",
      "           2   0.736715  0.595703  0.658747       512\n",
      "           3   0.442451  0.578125  0.501270       512\n",
      "           4   0.928416  0.835938  0.879753       512\n",
      "           5   0.550600  0.626953  0.586301       512\n",
      "           6   0.541339  0.537109  0.539216       512\n",
      "\n",
      "    accuracy                       0.649833      3584\n",
      "   macro avg   0.667264  0.649833  0.654879      3584\n",
      "weighted avg   0.667264  0.649833  0.654879      3584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_shift, scaler_shift = \\\n",
    "#     build_fpr_templates_ml(traces_mul_dict, start_end_indices['mul_shift'],\n",
    "#                            test_split, class_label_map_shift, 100)\n",
    "model_4_5, scaler_4_5, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_4_5, 100)\n",
    "model_6_7, scaler_6_7, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_6_7, 100)\n",
    "model_8_11, scaler_8_11, y_prob  = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_8_11, 200)\n",
    "model_12_15, scaler_12_15, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_12_15, 100)\n",
    "model_16_22, scaler_16_22, y_prob = \\\n",
    "    build_fpr_templates_ml(traces_mul_dict, range(len_mul),\n",
    "                           test_split, class_label_map_16_22, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e1582c-2138-4525-a01c-eed58510acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class_map = {\n",
    "    0: [0],\n",
    "    1: [1],\n",
    "    2: [-1],\n",
    "    3: [2, 3],\n",
    "    4: [-2, -3],\n",
    "    5: [4, 5, 6, 7],\n",
    "    6: [-4, -5, -6, -7],\n",
    "    7: list(range(8, 16)),\n",
    "    8: list(range(-15, -7)),\n",
    "    9: list(range(16, 32)),\n",
    "    10: list(range(-31, -15)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0963618-bc60-41c2-9eca-5d0df19050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class_abs_map = {\n",
    "    0: [0],\n",
    "    1: [1],\n",
    "    2: [2, 3],\n",
    "    3: [4, 5, 6, 7],\n",
    "    4: list(range(8, 16)),\n",
    "    5: list(range(16, 32)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d54564-60f8-4a94-b91e-68560225c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_choose_value(prob_dict):\n",
    "    mle_tmp_f_dict={}\n",
    "    for k, prob_array in prob_dict.items():\n",
    "        log_prob_array = np.log(prob_array)\n",
    "        col_sum = np.sum(log_prob_array, axis=0)\n",
    "        max_index = np.argmax(col_sum)\n",
    "        mle_tmp_f_dict[k]=max_index\n",
    "    return mle_tmp_f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "766cb79d-07ef-4067-b195-89d58b9436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sign(guess_attack_scaled,falcon_n):\n",
    "   sign_dict = {}\n",
    "   for i in range(falcon_n):\n",
    "        ele = guess_attack_scaled[i]\n",
    "        if new_class_map[ele][0] == 0:\n",
    "            sign_dict[i] = 0\n",
    "        elif new_class_map[ele][0] > 0:\n",
    "            sign_dict[i] = 1\n",
    "        elif new_class_map[ele][0] < 0:\n",
    "            sign_dict[i] = -1\n",
    "   return sign_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea47e535-a93c-47cc-80d9-8ce7587966ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_abs_table_index(guess_attack_scaled, falcon_n):\n",
    "    abs_dict = {}\n",
    "    for i in range(falcon_n):\n",
    "        ele = guess_attack_scaled[i]\n",
    "        if ele == 0:\n",
    "            abs_dict[i] = 0\n",
    "        elif ele == 1 or ele == 2:\n",
    "            abs_dict[i] = 1\n",
    "        elif ele == 3 or ele == 4:\n",
    "            abs_dict[i] = 2\n",
    "        elif ele == 5 or ele == 6:\n",
    "            abs_dict[i] = 3\n",
    "        elif ele == 7 or ele == 8:\n",
    "            abs_dict[i] = 4\n",
    "        elif ele == 9 or ele == 10:\n",
    "            abs_dict[i] = 5   \n",
    "    return abs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8db0fb0-f934-4c3a-b148-03ca66ba2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reorder_traces(A):\n",
    "    \"\"\"\n",
    "    Adjust the row order of A from [128,384,128,384,...] to [128,128,129,129,...,511,511]\n",
    "    \"\"\"\n",
    "    small = A[0::2, :]\n",
    "    large = A[1::2, :] \n",
    "    A_new = np.vstack([small, large])\n",
    "    return A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab09a582-758f-460b-9398-f70765ca2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffset_label(co_labels, target_filtered, set0):\n",
    "    label_diffset = []\n",
    "    for c in target_filtered:\n",
    "        if co_labels[c] in set0 or -co_labels[c] in set0:\n",
    "            label_diffset.append(0)\n",
    "        else: \n",
    "            label_diffset.append(1)\n",
    "    label_diffset = np.array(label_diffset)\n",
    "    return label_diffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce7a605d-0894-4100-82ce-a415a8ee18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_rows_by_labels(M, labels, subset):\n",
    "    \"\"\"\n",
    "    Select rows from a 2D array M, with row labels provided by `labels`.\n",
    "    \n",
    "    Parameters:  \n",
    "    M: numpy.array, shape = (n, m)\n",
    "    labels: list or array, length = n, the labels for each row in M\n",
    "    subset: list or set, the desired subset of labels\n",
    "    \n",
    "    Returns:\n",
    "    N: numpy.array, shape = (len(subset), m),\n",
    "    The row order is the same as in the subset.\n",
    "    \"\"\"\n",
    "    labels = np.array(labels)\n",
    "    subset = list(subset)\n",
    "    idx = [np.where(labels == s)[0][0] for s in subset]\n",
    "    N = M[idx, :]\n",
    "    return N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d8d24e4-3804-484c-8428-01eb3ff10ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fill_dict(to_fill_dict, prob_dict):\n",
    "    for kk, prob_list in prob_dict.items():\n",
    "        if kk not in to_fill_dict:\n",
    "            to_fill_dict[kk] = prob_list\n",
    "        else:\n",
    "            to_fill_dict[kk] = np.vstack([to_fill_dict[kk], prob_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d5fc2a-fb26-4ace-8058-29438b5a2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_process(fpr_diffset_mle_prob_list):\n",
    "    fpr_diffset_mle_prob_list = dict(fpr_diffset_mle_prob_list)\n",
    "    fpr_diffset_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_diffset_mle_prob_list.items()}\n",
    "    return fpr_diffset_mle_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5e41b74-de52-4a34-a321-3228a71e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cal_per_mul_prob(sets_dict, model_p,scaler_p,list_range):\n",
    "    \"\"\"\n",
    "    Concatenates, computes, and splits the dictionary `{key: (2, n) array}` back into a dictionary.\n",
    "    Parameters:\n",
    "    `data_dict`: dict[int, np.ndarray]\n",
    "    Each value is a NumPy array of (2, n) values.\n",
    "    `func`: callable\n",
    "    A function that takes (2*m, n) as input and outputs (2*m, k) values.\n",
    "    \n",
    "    Returns:\n",
    "    `new_dict`: dict[int, np.ndarray]\n",
    "     Each value is a NumPy array of (2, k) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Concatenate the keys in order to form a large matrix.\n",
    "    keys = list(sets_dict.keys())\n",
    "    big_array = np.vstack([sets_dict[k] for k in keys])\n",
    "    \n",
    "    # 2. calculate\n",
    "    # tmp_labels = np.repeat(labels,2)\n",
    "    result = classifier_inference(big_array, list_range, model_p, scaler_p)\n",
    "    \n",
    "    # 3. Split back into dictionary\n",
    "    new_dict = {}\n",
    "    for idx, key in enumerate(keys):\n",
    "        start = idx * 2\n",
    "        end = start + 2\n",
    "        new_dict[key] = result[start:end, :]\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04649cc9-bbc8-4f8c-9ecd-1c28003e7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 1000 keys in the database; I only collected the attack curves for the first 100 keys.\n",
    "filename_f = \"./data_k/Falcon_f_512_1000.csv\" \n",
    "filename_g = \"./data_k/Falcon_g_512_1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cee89a96-ecec-4e26-a4d8-6d5c1bb0506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The passed-in `guess_part_list` consists of dictionaries for guessing categories {4,5} and {6,7}, respectively.\n",
    "def fill_guess_dict(guess_part_list,f_final_set_p,f_sign_dict_p):\n",
    "    for idx_guess, guess_p in enumerate(guess_part_list):\n",
    "        for key_, guess_v in guess_p.items():\n",
    "            if idx_guess == 0: \n",
    "                f_final_set_p[key_]={guess_v+4} \n",
    "            if idx_guess == 1:\n",
    "                f_final_set_p[key_]={guess_v+6} \n",
    "    sorted_f_final_set_p = dict(sorted(f_final_set_p.items()))\n",
    "    f_guess_dict_p={}\n",
    "    for key, val in f_sign_dict_p.items():\n",
    "        if val>=0:\n",
    "            f_guess_dict_p[key] = sorted_f_final_set_p[key]\n",
    "        else:\n",
    "            f_guess_dict_p[key] = {-x for x in sorted_f_final_set_p[key]}\n",
    "    return f_guess_dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c30965e-ee7b-4fcd-bbd1-aef62525565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def attack_key_repeat(attack_scaled, attack_shift, attack_mul, mle_N):\n",
    "    #########First, process the data from fpr_scaled.\n",
    "    attack_traces_scaled = attack_scaled.reshape(-1,484) #The number of trace samples collected in fpr_scaled is 484, (N_attack_trace*512, 484).\n",
    "    attack_traces_scaled_lpf = trace_butter_lpf(attack_traces_scaled, sample_freq, cutoff_freq) #(N_attack_trace*512,484)\n",
    "    attack_traces_scaled_lpf = attack_traces_scaled_lpf.reshape(-1,512,484)\n",
    "    \n",
    "    fpr_scaled_mle_prob_list = defaultdict(list)\n",
    "    for i in range(mle_N):\n",
    "        prob_fpr_scaled = classifier_inference(attack_traces_scaled_lpf[i,:,:], list(range(484)), model_scaled, scaler_scaled)\n",
    "        #The dimensions of prob_fpr_scaled are (512, 11). Each coefficient's index is used as a key, and the corresponding prob values \n",
    "        #for N_attack_trace times are stored in the list corresponding to that key.\n",
    "        for j in range(512):\n",
    "            fpr_scaled_mle_prob_list[j].append(prob_fpr_scaled[j])\n",
    "    #A dictionary where key-value pairs correspond to coefficient indices, and each key-value pair contains mle_N probabilities of 11 categories.\n",
    "    fpr_scaled_mle_prob_list = dict(fpr_scaled_mle_prob_list)\n",
    "    fpr_scaled_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_scaled_mle_prob_list.items()}\n",
    "    scaled_dict = mle_choose_value(fpr_scaled_mle_prob_list)\n",
    "    \n",
    "    #Record information after attacking fpr_scaled\n",
    "    sign_dict = record_sign(scaled_dict,512)\n",
    "    abs_dict = record_abs_table_index(scaled_dict,512)\n",
    "    # print(abs_dict)\n",
    "\n",
    "    ######### Process the trace data corresponding to the normalization procedure in fpr_mul\n",
    "    #f_128~f_255; f_384~f_511 will involve multiplication operations. We need to analyze these coefficients, \n",
    "    #identifying which are already 0, -1, or 1, and which require further differentiation.\n",
    "    check_keys = list(range(128, 256)) + list(range(384, 512))\n",
    "    keys_012 = [k for k in check_keys if scaled_dict[k] in (0, 1, 2)]\n",
    "    # keys_other = [k for k in check_keys if scaled_dict[k] not in (0, 1, 2)]\n",
    "  \n",
    "    # The coefficient indices of traces in `attack_traces_shift` are:\n",
    "    # f_128,f_384,f_128,f_384,f_129,f_385,f_129,f_384,...,f_255,f_511,f_255,f_511\n",
    "    # After processing, the coefficient indices of traces in `attack_traces_shift` are \n",
    "    # f_128,f_128,f_129,f129...,f_255,f255,f_384,f384,f_385,f385...,f_511,f511\n",
    "    # At this point, the `mle_N` observations should be utilized.\n",
    "    set0 = {2, 4, 5, 8, 9, 10, 11, 16, 17, 18, 19, 20, 21, 22}\n",
    "    set1 = {3, 6, 7, 12, 13, 14, 15, 23, 24, 25, 26, 27, 28, 29, 30, 31}\n",
    "    target_order = list(range(128, 256)) + list(range(384, 512))\n",
    "    target_filtered = [c for c in target_order if c not in keys_012]\n",
    "    keep_rows = [i for i, c in enumerate(target_order) if c not in keys_012]\n",
    "    \n",
    "    fpr_shift_mle_prob_list = defaultdict(list)\n",
    "    \n",
    "    for i in range(mle_N):\n",
    "        attack_traces_shift = reorder_traces(attack_shift[i,:,:]) \n",
    "        attack_traces_shift_lpf = trace_butter_lpf(attack_traces_shift, sample_freq, cutoff_freq)\n",
    "        attack_traces_shift_lpf_1 = attack_traces_shift_lpf[0::2] \n",
    "        attack_traces_shift_lpf_2 = attack_traces_shift_lpf[1::2]\n",
    "    \n",
    "        # Continuing with the processing of attack_traces_shift_lpf, since the coefficient indices in keys_012 are already determined, \n",
    "        # no further processing is needed; therefore, this part of the data is filtered out.\n",
    "        attack_traces_shift_filtered_1 = attack_traces_shift_lpf_1[keep_rows, :]\n",
    "        attack_traces_shift_filtered_2 = attack_traces_shift_lpf_2[keep_rows, :]\n",
    "\n",
    "        prob_fpr_shift_1 = classifier_inference(attack_traces_shift_filtered_1, list(range(88)), model_shift, scaler_shift)\n",
    "        # print(prob_fpr_diffset_1)\n",
    "        prob_fpr_shift_2 = classifier_inference(attack_traces_shift_filtered_2, list(range(88)), model_shift, scaler_shift)\n",
    "        # print(prob_fpr_diffset_2)\n",
    "        \n",
    "        for j in range(len(target_filtered)):\n",
    "            fpr_shift_mle_prob_list[target_filtered[j]].append(prob_fpr_shift_1[j])\n",
    "            fpr_shift_mle_prob_list[target_filtered[j]].append(prob_fpr_shift_2[j])\n",
    "    \n",
    "    # A dictionary where key-value pairs correspond to coefficient indices, and \n",
    "    # each key-value pair contains 2*N_attack_trace values representing binary classification probabilities.\n",
    "    fpr_shift_mle_prob_list = dict(fpr_shift_mle_prob_list)\n",
    "    fpr_shift_mle_prob_list = {key_coeff: np.array(val_list) for key_coeff, val_list in fpr_shift_mle_prob_list.items()}\n",
    "    shift_dict = mle_choose_value(fpr_shift_mle_prob_list)#mle得到标签值\n",
    "    # print(shift_dict)\n",
    "\n",
    "    ######Based on the attack results of fpr_scaled and the normalization procedure in fpr_mul, \n",
    "    # the initial range of values for the partitioning coefficients is determined.\n",
    "    final_set = {}\n",
    "    sure_set=[]\n",
    "    insure_set=[]\n",
    "    for key, value in abs_dict.items():\n",
    "        if value == 0 or value == 1:\n",
    "            sure_set.append(key)\n",
    "            final_set[key]=set(new_class_abs_map[value])\n",
    "        else:\n",
    "            insure_set.append(key)\n",
    "    # Taking the intersection of the sets determined by the fpr_scaled and the normalization procedure in fpr_mul \n",
    "    # can determine the absolute values of 2 and 3, but only for coefficients multiplied by the rotation factor.\n",
    "    for insure_idx in insure_set:\n",
    "        if insure_idx in target_filtered:\n",
    "            # label_choose = guess_attack_diffset[target_filtered_f.index(insure_idx)]\n",
    "            label_choose = shift_dict[insure_idx]\n",
    "            if label_choose == 0:\n",
    "                tmp_set = set0 & set(new_class_abs_map[abs_dict[insure_idx]])\n",
    "                final_set[insure_idx]=tmp_set\n",
    "            else:\n",
    "                tmp_set = set1 & set(new_class_abs_map[abs_dict[insure_idx]])\n",
    "                final_set[insure_idx]=tmp_set\n",
    "    # Add the range of coefficients that are not multiplied by the twitch factor and are not in the range of 0, 1, -1 to f_final_set.\n",
    "    no_mul_w = list(range(0, 128)) + list(range(256, 384))\n",
    "    no_mul_w_insure = [d for d in no_mul_w if d not in sure_set]\n",
    "    for d in no_mul_w_insure:\n",
    "        final_set[d] = set(new_class_abs_map[abs_dict[d]])\n",
    "\n",
    "    #########Begin dividing {4, 5} and {6, 7}\n",
    "    fpr_mul_mle_prob_list_45 = defaultdict(list)\n",
    "    fpr_mul_mle_prob_list_67 = defaultdict(list)\n",
    "    mul_idx_list=[]\n",
    "    #Retrieve the indices of coefficients multiplied by w that still do not have a definite value after using fpr_scaled and diffset.\n",
    "    mul_w = list(range(128, 256)) + list(range(384, 512))\n",
    "    for d in mul_w:\n",
    "        if len(final_set[d]) != 1:\n",
    "            mul_idx_list.append(d)\n",
    "\n",
    "    # mul_labels = pd.read_csv(filename_g, header=None, skiprows=key_idx, nrows=1)#读取标签，header=None不要省略\n",
    "    # print(scaled_right_idx_list[key_th])\n",
    "    # mul_labels = mul_labels.iloc[0]\n",
    "    # mul_labels = np.array(mul_labels)\n",
    "\n",
    "    for i in range(mle_N):\n",
    "        \n",
    "        attack_traces_mul = reorder_traces(attack_mul[i,:,:])\n",
    "        attack_traces_mul_1 = attack_traces_mul[0::2] \n",
    "        attack_traces_mul_2 = attack_traces_mul[1::2] \n",
    "        attack_traces_mul_lpf_1 = trace_butter_lpf(attack_traces_mul_1, sample_freq, cutoff_freq)\n",
    "        attack_traces_mul_lpf_2 = trace_butter_lpf(attack_traces_mul_2, sample_freq, cutoff_freq)\n",
    "       \n",
    "        attack_traces_mul_lpf_1 = select_rows_by_labels(attack_traces_mul_lpf_1, mul_w, mul_idx_list)\n",
    "        attack_traces_mul_lpf_2 = select_rows_by_labels(attack_traces_mul_lpf_2, mul_w, mul_idx_list)\n",
    "        \n",
    "        \n",
    "        sets_4_5 = defaultdict(list)\n",
    "        sets_6_7 = defaultdict(list)  \n",
    "       \n",
    "        # labels_4_5=[]\n",
    "        # print(f_final_set)\n",
    "        #Divide into different sets\n",
    "        for i in range(len(mul_idx_list)):\n",
    "            if final_set[mul_idx_list[i]] == {4,5}:\n",
    "                sets_4_5[mul_idx_list[i]].append(attack_traces_mul_lpf_1[i])\n",
    "                sets_4_5[mul_idx_list[i]].append(attack_traces_mul_lpf_2[i])\n",
    "                # labels_4_5.append(abs(mul_labels[mul_idx_list[i]])-4)\n",
    "            elif final_set[mul_idx_list[i]] == {6,7}:\n",
    "                sets_6_7[mul_idx_list[i]].append(attack_traces_mul_lpf_1[i])\n",
    "                sets_6_7[mul_idx_list[i]].append(attack_traces_mul_lpf_2[i])\n",
    "               \n",
    "\n",
    "        sets_4_5 = dict_process(sets_4_5)\n",
    "        sets_6_7 = dict_process(sets_6_7)\n",
    "        # labels_4_5 = np.array(labels_4_5)\n",
    "   \n",
    "        prob_fpr_mul_4_5 = cal_per_mul_prob(sets_4_5,model_4_5,scaler_4_5,list(range(68)))\n",
    "        prob_fpr_mul_6_7 = cal_per_mul_prob(sets_6_7,model_6_7,scaler_6_7,list(range(68))) \n",
    "        # print(prob_fpr_mul_4_5)\n",
    "            \n",
    "        fill_dict(fpr_mul_mle_prob_list_45, prob_fpr_mul_4_5)\n",
    "        fill_dict(fpr_mul_mle_prob_list_67, prob_fpr_mul_6_7)\n",
    "       \n",
    "    fpr_mul_mle_prob_list_45 = dict_process(fpr_mul_mle_prob_list_45)\n",
    "    fpr_mul_mle_prob_list_67 = dict_process(fpr_mul_mle_prob_list_67)\n",
    "    # print(fpr_mul_mle_prob_list_45)\n",
    "    \n",
    "    guess_45 = mle_choose_value(fpr_mul_mle_prob_list_45)\n",
    "    guess_67 = mle_choose_value(fpr_mul_mle_prob_list_67)\n",
    "    # print(guess_45)\n",
    "\n",
    "    part_guess_list=[guess_45, guess_67]\n",
    "    final_guess_dict = fill_guess_dict(part_guess_list,final_set,sign_dict)\n",
    "            \n",
    "    return final_guess_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3ed5643-33fe-4ab2-8676-8c14392316b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge f and g into f||g, first check if the number of currently \"determined\" coefficients (with only one candidate value) isgreater than or equal to 512.\n",
    "# If less than 512, fail directly.\n",
    "# If equal to 512, check if these 512 are correct.\n",
    "# If greater than 512, set it as num_guess, and randomly select 512 from num_guess each time, continuously checking if they are correct. If none of these attempts succeed, then it's a failure.\n",
    "\n",
    "#The extraction principle is to prioritize keeping values that are definitely 0 or ±1, followed by ±2 or ±3, then ±4 or ±5, and finally ±6 or ±7.\n",
    "\n",
    "#Return values: -1: The number of confirmed values is less than 512; -2: Exactly 512 values are confirmed, but there are errors; -3: len(candidate) < 512 - need;\n",
    "#-4: Multiple attempts failed. 1: Exactly 512 values succeeded; 2: Success after random sampling; 3: Success when getting to {4, 5}.\n",
    "\n",
    "\n",
    "def check_key_right(f_guess_dict_p, g_guess_dict_p, filename_f_p, filename_g_p, kk_idx, attempts):\n",
    "   \n",
    "    import csv, random\n",
    "    total_num = 0\n",
    "\n",
    "    # --- Step1---\n",
    "    f_determined = {k: list(v)[0] for k, v in f_guess_dict_p.items() if len(v) == 1}\n",
    "    g_determined = {k: list(v)[0] for k, v in g_guess_dict_p.items() if len(v) == 1}\n",
    "    total_determined = len(f_determined) + len(g_determined)\n",
    "\n",
    "    if total_determined < 512:\n",
    "        return -1,1\n",
    "\n",
    "    def read_row(csv_path, row_idx):\n",
    "        with open(csv_path, newline=\"\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for j, row in enumerate(reader):\n",
    "                if j == row_idx:\n",
    "                    return list(map(int, row))\n",
    "        raise ValueError(f\"Row {row_idx} not found in {csv_path}\")\n",
    "\n",
    "    f_row = read_row(filename_f_p, kk_idx)\n",
    "    g_row = read_row(filename_g_p, kk_idx)\n",
    "\n",
    "    # --- Step2: If there are exactly 512 definite positions ---\n",
    "    if total_determined == 512:\n",
    "        for k, v in f_determined.items():\n",
    "            if f_row[k] != v:\n",
    "                return -2\n",
    "        for k, v in g_determined.items():\n",
    "            if g_row[k] != v:\n",
    "                return -2\n",
    "        return 1, 1\n",
    "\n",
    "    \n",
    "    # --- Step2.5: Prioritize checking the position of values in the range {0, ±1, ±2, ±3, ±4, ±5}. ---\n",
    "    # This step is equivalent to the procedure used in the -O0 level experiment. \n",
    "    f_dict_p = {}\n",
    "    g_dict_p = {}\n",
    "  \n",
    "    special_values = {0, 1, -1, 2, -2, 3, -3, 4, -4, 5, -5}\n",
    "    priority_positions = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v in special_values:\n",
    "            priority_positions.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v in special_values:\n",
    "            priority_positions.append((\"g\", k, v))\n",
    "\n",
    "    if len(priority_positions) >= 512:\n",
    "        print(\"jinru\")\n",
    "        ok = True\n",
    "        for src, k, v in priority_positions:\n",
    "            if src == \"f\":\n",
    "                if f_row[k] != v:\n",
    "                    # print(\"f\")\n",
    "                    # print(k)\n",
    "                    # print(f_row[k])\n",
    "                    # print(v)\n",
    "                    ok = False\n",
    "                    break\n",
    "            else:  # \"g\"\n",
    "                if g_row[k] != v:\n",
    "                    # print(\"g\")\n",
    "                    # print(k)\n",
    "                    ok = False\n",
    "                    break\n",
    "        if ok:\n",
    "            for source, k, v in priority_positions:\n",
    "                if source == \"f\":\n",
    "                    f_dict_p[k] = v\n",
    "                elif source == \"g\":\n",
    "                    g_dict_p[k] = v\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon512/f_{kk_idx}_guess_try.pkl\", \"wb\") as f:\n",
    "                pickle.dump(f_dict_p, f)\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon512/g_{kk_idx}_guess_try.pkl\", \"wb\") as g:\n",
    "                pickle.dump(g_dict_p, g)        \n",
    "            return (3, 1)\n",
    "    # If it fails, continue with random attempts\n",
    "    \n",
    "    # --- Step3: total_determined > 512 ---\n",
    "    special_positions = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v in (0, 1, -1):\n",
    "            special_positions.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v in (0, 1, -1):\n",
    "            special_positions.append((\"g\", k, v))\n",
    "\n",
    "    m = len(special_positions)\n",
    "\n",
    "    candidates = []\n",
    "    for k, v in f_determined.items():\n",
    "        if v not in (0, 1, -1):\n",
    "            candidates.append((\"f\", k, v))\n",
    "    for k, v in g_determined.items():\n",
    "        if v not in (0, 1, -1):\n",
    "            candidates.append((\"g\", k, v))\n",
    "\n",
    "    need = 512 - m\n",
    "    print(f\"need is {need}\")\n",
    "    print(f\"len of candidates is {len(candidates)}\")\n",
    "    if need < 0 or len(candidates) < need:\n",
    "        return -3, 1\n",
    "\n",
    "    # attempts = 1e6  # can adjust\n",
    "    for _ in range(attempts):\n",
    "        total_num = total_num + 1\n",
    "        sample = random.sample(candidates, need)\n",
    "        positions = special_positions + sample\n",
    "\n",
    "        ok = True\n",
    "        for src, k, v in positions:\n",
    "            if src == \"f\":\n",
    "                if f_row[k] != v:\n",
    "                    ok = False\n",
    "                    break\n",
    "            else:  # src == \"g\"\n",
    "                if g_row[k] != v:\n",
    "                    ok = False\n",
    "                    break\n",
    "        if ok:\n",
    "            for source, k, v in positions:\n",
    "                if source == \"f\":\n",
    "                    f_dict_p[k] = v\n",
    "                elif source == \"g\":\n",
    "                    g_dict_p[k] = v\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon512/f_{kk_idx}_guess_try.pkl\", \"wb\") as f:\n",
    "                pickle.dump(f_dict_p, f)\n",
    "            with open(f\"./data/data_k_guess_isd/Falcon512/g_{kk_idx}_guess_try.pkl\", \"wb\") as g:\n",
    "                pickle.dump(g_dict_p, g)       \n",
    "            return 2, total_num\n",
    "\n",
    "    return -4, 1e6\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44cd0336-6dec-4a0a-8174-5c57d7e6c915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jinru\n",
      "need is 228\n",
      "len of candidates is 334\n",
      "the 0th key guess's state is 2\n",
      "jinru\n",
      "need is 203\n",
      "len of candidates is 317\n",
      "the 1th key guess's state is 2\n",
      "jinru\n",
      "need is 236\n",
      "len of candidates is 355\n",
      "the 2th key guess's state is 2\n",
      "jinru\n",
      "need is 213\n",
      "len of candidates is 326\n",
      "the 3th key guess's state is -4\n",
      "jinru\n",
      "need is 189\n",
      "len of candidates is 306\n",
      "the 4th key guess's state is 2\n",
      "jinru\n",
      "need is 211\n",
      "len of candidates is 336\n",
      "the 5th key guess's state is -4\n",
      "jinru\n",
      "need is 223\n",
      "len of candidates is 335\n",
      "the 6th key guess's state is 2\n",
      "jinru\n",
      "need is 225\n",
      "len of candidates is 339\n",
      "the 7th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 327\n",
      "the 8th key guess's state is 2\n",
      "jinru\n",
      "need is 201\n",
      "len of candidates is 325\n",
      "the 9th key guess's state is -4\n",
      "jinru\n",
      "need is 207\n",
      "len of candidates is 330\n",
      "the 10th key guess's state is 2\n",
      "jinru\n",
      "need is 219\n",
      "len of candidates is 337\n",
      "the 11th key guess's state is -4\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 326\n",
      "the 12th key guess's state is -4\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 314\n",
      "the 13th key guess's state is 2\n",
      "jinru\n",
      "need is 237\n",
      "len of candidates is 336\n",
      "the 14th key guess's state is 2\n",
      "jinru\n",
      "need is 225\n",
      "len of candidates is 344\n",
      "the 15th key guess's state is 2\n",
      "jinru\n",
      "need is 204\n",
      "len of candidates is 317\n",
      "the 16th key guess's state is 2\n",
      "jinru\n",
      "need is 222\n",
      "len of candidates is 341\n",
      "the 17th key guess's state is 2\n",
      "jinru\n",
      "need is 219\n",
      "len of candidates is 332\n",
      "the 18th key guess's state is 2\n",
      "jinru\n",
      "need is 215\n",
      "len of candidates is 329\n",
      "the 19th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 318\n",
      "the 20th key guess's state is 2\n",
      "jinru\n",
      "need is 220\n",
      "len of candidates is 336\n",
      "the 21th key guess's state is 2\n",
      "jinru\n",
      "need is 201\n",
      "len of candidates is 327\n",
      "the 22th key guess's state is 2\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 323\n",
      "the 23th key guess's state is 2\n",
      "jinru\n",
      "need is 234\n",
      "len of candidates is 341\n",
      "the 24th key guess's state is 2\n",
      "jinru\n",
      "need is 228\n",
      "len of candidates is 333\n",
      "the 25th key guess's state is -4\n",
      "jinru\n",
      "need is 201\n",
      "len of candidates is 317\n",
      "the 26th key guess's state is -4\n",
      "jinru\n",
      "need is 194\n",
      "len of candidates is 340\n",
      "the 27th key guess's state is -4\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 330\n",
      "the 28th key guess's state is 2\n",
      "jinru\n",
      "need is 220\n",
      "len of candidates is 323\n",
      "the 29th key guess's state is 2\n",
      "jinru\n",
      "need is 209\n",
      "len of candidates is 340\n",
      "the 30th key guess's state is 2\n",
      "jinru\n",
      "need is 217\n",
      "len of candidates is 339\n",
      "the 31th key guess's state is 2\n",
      "jinru\n",
      "need is 220\n",
      "len of candidates is 339\n",
      "the 32th key guess's state is 2\n",
      "jinru\n",
      "need is 188\n",
      "len of candidates is 322\n",
      "the 33th key guess's state is 2\n",
      "jinru\n",
      "need is 215\n",
      "len of candidates is 334\n",
      "the 34th key guess's state is 2\n",
      "jinru\n",
      "need is 210\n",
      "len of candidates is 328\n",
      "the 35th key guess's state is 2\n",
      "jinru\n",
      "need is 217\n",
      "len of candidates is 333\n",
      "the 36th key guess's state is 2\n",
      "jinru\n",
      "need is 221\n",
      "len of candidates is 336\n",
      "the 37th key guess's state is 2\n",
      "jinru\n",
      "need is 186\n",
      "len of candidates is 316\n",
      "the 38th key guess's state is 2\n",
      "jinru\n",
      "need is 196\n",
      "len of candidates is 321\n",
      "the 39th key guess's state is -4\n",
      "jinru\n",
      "need is 198\n",
      "len of candidates is 314\n",
      "the 40th key guess's state is 2\n",
      "jinru\n",
      "need is 207\n",
      "len of candidates is 330\n",
      "the 41th key guess's state is -4\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 344\n",
      "the 42th key guess's state is 2\n",
      "jinru\n",
      "need is 231\n",
      "len of candidates is 337\n",
      "the 43th key guess's state is 2\n",
      "jinru\n",
      "need is 216\n",
      "len of candidates is 331\n",
      "the 44th key guess's state is 2\n",
      "jinru\n",
      "need is 210\n",
      "len of candidates is 334\n",
      "the 45th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 327\n",
      "the 46th key guess's state is 2\n",
      "jinru\n",
      "need is 226\n",
      "len of candidates is 342\n",
      "the 47th key guess's state is 2\n",
      "jinru\n",
      "need is 216\n",
      "len of candidates is 337\n",
      "the 48th key guess's state is 2\n",
      "jinru\n",
      "need is 221\n",
      "len of candidates is 342\n",
      "the 49th key guess's state is -4\n",
      "jinru\n",
      "need is 222\n",
      "len of candidates is 329\n",
      "the 50th key guess's state is 2\n",
      "jinru\n",
      "need is 205\n",
      "len of candidates is 342\n",
      "the 51th key guess's state is 2\n",
      "jinru\n",
      "need is 215\n",
      "len of candidates is 317\n",
      "the 52th key guess's state is 2\n",
      "jinru\n",
      "need is 180\n",
      "len of candidates is 319\n",
      "the 53th key guess's state is 2\n",
      "jinru\n",
      "need is 208\n",
      "len of candidates is 332\n",
      "the 54th key guess's state is -4\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 320\n",
      "the 55th key guess's state is -4\n",
      "jinru\n",
      "need is 200\n",
      "len of candidates is 329\n",
      "the 56th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 318\n",
      "the 57th key guess's state is 2\n",
      "jinru\n",
      "need is 215\n",
      "len of candidates is 318\n",
      "the 58th key guess's state is 2\n",
      "jinru\n",
      "need is 229\n",
      "len of candidates is 330\n",
      "the 59th key guess's state is -4\n",
      "jinru\n",
      "need is 197\n",
      "len of candidates is 331\n",
      "the 60th key guess's state is 2\n",
      "jinru\n",
      "need is 252\n",
      "len of candidates is 359\n",
      "the 61th key guess's state is 2\n",
      "jinru\n",
      "need is 222\n",
      "len of candidates is 335\n",
      "the 62th key guess's state is -4\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 338\n",
      "the 63th key guess's state is 2\n",
      "jinru\n",
      "need is 232\n",
      "len of candidates is 346\n",
      "the 64th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 345\n",
      "the 65th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 341\n",
      "the 66th key guess's state is 2\n",
      "jinru\n",
      "need is 183\n",
      "len of candidates is 317\n",
      "the 67th key guess's state is 2\n",
      "jinru\n",
      "need is 215\n",
      "len of candidates is 317\n",
      "the 68th key guess's state is 2\n",
      "jinru\n",
      "need is 203\n",
      "len of candidates is 324\n",
      "the 69th key guess's state is 2\n",
      "jinru\n",
      "need is 225\n",
      "len of candidates is 337\n",
      "the 70th key guess's state is 2\n",
      "jinru\n",
      "need is 221\n",
      "len of candidates is 327\n",
      "the 71th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 324\n",
      "the 72th key guess's state is 2\n",
      "jinru\n",
      "need is 212\n",
      "len of candidates is 319\n",
      "the 73th key guess's state is 2\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 335\n",
      "the 74th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 320\n",
      "the 75th key guess's state is 2\n",
      "jinru\n",
      "need is 200\n",
      "len of candidates is 326\n",
      "the 76th key guess's state is 2\n",
      "jinru\n",
      "need is 202\n",
      "len of candidates is 329\n",
      "the 77th key guess's state is 2\n",
      "jinru\n",
      "need is 209\n",
      "len of candidates is 327\n",
      "the 78th key guess's state is 2\n",
      "jinru\n",
      "need is 218\n",
      "len of candidates is 340\n",
      "the 79th key guess's state is -4\n",
      "jinru\n",
      "need is 188\n",
      "len of candidates is 319\n",
      "the 80th key guess's state is 2\n",
      "jinru\n",
      "need is 196\n",
      "len of candidates is 331\n",
      "the 81th key guess's state is 2\n",
      "jinru\n",
      "need is 189\n",
      "len of candidates is 310\n",
      "the 82th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 339\n",
      "the 83th key guess's state is -4\n",
      "jinru\n",
      "need is 211\n",
      "len of candidates is 332\n",
      "the 84th key guess's state is 2\n",
      "jinru\n",
      "need is 214\n",
      "len of candidates is 336\n",
      "the 85th key guess's state is -4\n",
      "jinru\n",
      "need is 219\n",
      "len of candidates is 328\n",
      "the 86th key guess's state is 2\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 318\n",
      "the 87th key guess's state is -4\n",
      "jinru\n",
      "need is 204\n",
      "len of candidates is 327\n",
      "the 88th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 318\n",
      "the 89th key guess's state is 2\n",
      "jinru\n",
      "need is 221\n",
      "len of candidates is 331\n",
      "the 90th key guess's state is 2\n",
      "jinru\n",
      "need is 187\n",
      "len of candidates is 314\n",
      "the 91th key guess's state is 2\n",
      "jinru\n",
      "need is 199\n",
      "len of candidates is 306\n",
      "the 92th key guess's state is -4\n",
      "jinru\n",
      "need is 206\n",
      "len of candidates is 317\n",
      "the 93th key guess's state is 2\n",
      "jinru\n",
      "need is 238\n",
      "len of candidates is 341\n",
      "the 94th key guess's state is 2\n",
      "jinru\n",
      "need is 212\n",
      "len of candidates is 340\n",
      "the 95th key guess's state is 2\n",
      "jinru\n",
      "need is 196\n",
      "len of candidates is 329\n",
      "the 96th key guess's state is 2\n",
      "jinru\n",
      "need is 235\n",
      "len of candidates is 344\n",
      "the 97th key guess's state is 2\n",
      "jinru\n",
      "need is 220\n",
      "len of candidates is 337\n",
      "the 98th key guess's state is 2\n",
      "jinru\n",
      "need is 211\n",
      "len of candidates is 330\n",
      "the 99th key guess's state is 2\n",
      "the succ count is 80\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "random.seed(2025)\n",
    "mle_N = 5\n",
    "try_random_num = 1000000\n",
    "# succ_num_count=0\n",
    "num_attack_key=100\n",
    "key_key_idx_list=[]\n",
    "error_list=[]\n",
    "# succ_num_list=[]\n",
    "succ_num_count=0\n",
    "check_num_list=[] \n",
    "for key_th in range(num_attack_key):\n",
    "    # the attack trace segments corresponding to the fpr_scaled, normalization procedure and mantissa multiplication in fpr_mul for the f key\n",
    "    attack_traces_scaled_f = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_scaled/{key_th}th_f_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_diffset_f = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_mul/{key_th}th_f_shift_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_multi_f = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_mul/{key_th}th_f_multiply_50traces.npy\")[:mle_N,:,:]\n",
    "\n",
    "    # the attack trace segments corresponding to the fpr_scaled, normalization procedure and mantissa multiplication in fpr_mul for the g key\n",
    "    attack_traces_scaled_g = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_scaled/{key_th}th_g_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_diffset_g = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_mul/{key_th}th_g_shift_50traces.npy\")[:mle_N,:,:]\n",
    "    attack_traces_multi_g = np.load(f\"./data-O3-828/falcon512_attack_latest/fpr_mul/{key_th}th_g_multiply_50traces.npy\")[:mle_N,:,:]\n",
    "\n",
    "    \n",
    "    #Estimate a portion of the key for f, continuing until {4,5} and {6,7} are partitioned.\n",
    "    f_guess_dict = attack_key_repeat(attack_traces_scaled_f,attack_traces_diffset_f,attack_traces_multi_f,mle_N)\n",
    "    g_guess_dict = attack_key_repeat(attack_traces_scaled_g,attack_traces_diffset_g,attack_traces_multi_g,mle_N)\n",
    "    \n",
    "    re, check_num = check_key_right(f_guess_dict, g_guess_dict, filename_f, filename_g, key_th, try_random_num)\n",
    "    print(f\"the {key_th}th key guess's state is {re}\")\n",
    "    if re>0:\n",
    "        succ_num_count = succ_num_count + 1\n",
    "        key_key_idx_list.append(key_th)\n",
    "        check_num_list.append(check_num)\n",
    "    else:\n",
    "        error_list.append(key_th)\n",
    "print(f\"the succ count is {succ_num_count}\")\n",
    "# np.save(\"./data/data_k_guess_isd/Falcon512/key_idx_list_try.npy\", key_key_idx_list)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
